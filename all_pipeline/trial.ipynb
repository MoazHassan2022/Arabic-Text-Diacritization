{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAdABNYOVySK",
        "outputId": "3ed3fd4d-2d0c-44ca-9f59-f942bdd05c21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfWxOXMuW8ef",
        "outputId": "c495fbd7-b51c-4b18-e237-9ea84f4c7f98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content\n",
            "/content\n",
            "Arabic-Text-Diacritization  sample_data\n",
            "sample_data\n"
          ]
        }
      ],
      "source": [
        "!pwd\n",
        "%cd /content\n",
        "!pwd\n",
        "!ls\n",
        "!rm -rf Arabic-Text-Diacritization\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKM1tsD3ZVLZ",
        "outputId": "5f1cbbf9-e7be-46dc-e9d7-e7867531c7f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Arabic-Text-Diacritization'...\n",
            "warning: redirecting to https://github.com/MoazHassan2022/Arabic-Text-Diacritization.git/\n",
            "remote: Enumerating objects: 332, done.\u001b[K\n",
            "remote: Counting objects: 100% (47/47), done.\u001b[K\n",
            "remote: Compressing objects: 100% (37/37), done.\u001b[K\n",
            "remote: Total 332 (delta 22), reused 33 (delta 10), pack-reused 285\u001b[K\n",
            "Receiving objects: 100% (332/332), 23.88 MiB | 7.61 MiB/s, done.\n",
            "Resolving deltas: 100% (148/148), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://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:@github.com/MoazHassan2022/Arabic-Text-Diacritization.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpJdAj95ebla",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a6fe320-94cb-46a9-b420-0ef4648f76b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "docljWYHaXKw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24d2ec88-994d-43d1-912f-0717da983c0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Arabic-Text-Diacritization/model/BLSTM\n"
          ]
        }
      ],
      "source": [
        "%cd /content/Arabic-Text-Diacritization/model/BLSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbTFh9AKabP1"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import re\n",
        "import sys\n",
        "import os\n",
        "import pickle\n",
        "sys.path.insert(0, os.path.abspath('../..'))\n",
        "from preprocessing.preprocess import *\n",
        "import textwrap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckPPX9MaZcrL"
      },
      "outputs": [],
      "source": [
        "def count_spaces(input_string):\n",
        "  space_count = len(re.findall(r'\\s', input_string))\n",
        "  return space_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODJ8RO4TalEk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b4433a6-1fe8-4fe2-bcc2-6a736bf4b482"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "56366\n",
            "2696\n",
            "56366\n",
            "2696\n",
            "قوله : أو قطع الأول يده إلخ قال الزركشي\n",
            "قَوْلُهُ : أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَخْ قَالَ الزَّرْكَشِيُّ\n",
            "{'د': 1, '؟': 2, 'آ': 3, 'إ': 4, 'ؤ': 5, 'ط': 6, 'م': 7, '،': 8, 'ة': 9, 'ت': 10, 'ر': 11, 'ئ': 12, 'ا': 13, 'ض': 14, '!': 15, ' ': 16, 'ك': 17, 'غ': 18, 'س': 19, 'ص': 20, 'أ': 21, 'ل': 22, 'ف': 23, 'ظ': 24, 'ج': 25, '؛': 26, 'ن': 27, 'ع': 28, 'ب': 29, 'ث': 30, 'ه': 31, 'خ': 32, 'ى': 33, 'ء': 34, 'ز': 35, 'ق': 36, 'ي': 37, 'ش': 38, 'ح': 39, ':': 40, 'ذ': 41, 'و': 42, '.': 43}\n",
            "{1: 'د', 2: '؟', 3: 'آ', 4: 'إ', 5: 'ؤ', 6: 'ط', 7: 'م', 8: '،', 9: 'ة', 10: 'ت', 11: 'ر', 12: 'ئ', 13: 'ا', 14: 'ض', 15: '!', 16: ' ', 17: 'ك', 18: 'غ', 19: 'س', 20: 'ص', 21: 'أ', 22: 'ل', 23: 'ف', 24: 'ظ', 25: 'ج', 26: '؛', 27: 'ن', 28: 'ع', 29: 'ب', 30: 'ث', 31: 'ه', 32: 'خ', 33: 'ى', 34: 'ء', 35: 'ز', 36: 'ق', 37: 'ي', 38: 'ش', 39: 'ح', 40: ':', 41: 'ذ', 42: 'و', 43: '.'}\n"
          ]
        }
      ],
      "source": [
        "dataset_path = '../../dataset'\n",
        "\n",
        "# preprocess and save the data\n",
        "preprocess_data(data_type='train', dataset_path=dataset_path)\n",
        "preprocess_data(data_type='val', dataset_path=dataset_path)\n",
        "\n",
        "max_len = 600\n",
        "\n",
        "# load data\n",
        "training_data = []\n",
        "training_spaces = []\n",
        "# load data\n",
        "with open(f'{dataset_path}/cleaned_train_data_without_diacritics.txt', 'r', encoding='utf-8') as file:\n",
        "    # read all lines into array of lines\n",
        "    training_data_lines = file.readlines()\n",
        "    for i in range(len(training_data_lines)):\n",
        "        training_data_lines[i] = re.compile(r'[\\n\\r\\t]').sub('', training_data_lines[i])\n",
        "        training_data_lines[i] = re.compile(r'\\s+').sub(' ', training_data_lines[i])\n",
        "        training_data_lines[i] = training_data_lines[i].strip()\n",
        "\n",
        "        # Split the line into sentences of max_len, without cutting words\n",
        "        sentences = textwrap.wrap(training_data_lines[i], max_len)\n",
        "\n",
        "        for sentence in sentences:\n",
        "            training_data.append(sentence)\n",
        "            training_spaces.append(count_spaces(sentence))\n",
        "\n",
        "# validation\n",
        "validation_data = []\n",
        "validation_spaces = []\n",
        "# load data\n",
        "with open(f'{dataset_path}/cleaned_val_data_without_diacritics.txt', 'r', encoding='utf-8') as file:\n",
        "    # read all lines into array of lines\n",
        "    validation_data_lines = file.readlines()\n",
        "    for i in range(len(validation_data_lines)):\n",
        "        validation_data_lines[i] = re.compile(r'[\\n\\r\\t]').sub('', validation_data_lines[i])\n",
        "        validation_data_lines[i] = re.compile(r'\\s+').sub(' ', validation_data_lines[i])\n",
        "        validation_data_lines[i] = validation_data_lines[i].strip()\n",
        "\n",
        "        # Split the line into sentences of max_len, without cutting words\n",
        "        sentences = textwrap.wrap(validation_data_lines[i], max_len)\n",
        "\n",
        "        for sentence in sentences:\n",
        "            validation_data.append(sentence)\n",
        "            validation_spaces.append(count_spaces(sentence))\n",
        "\n",
        "\n",
        "training_data_with_diacritics = []\n",
        "spaces_index = 0\n",
        "\n",
        "with open(f'{dataset_path}/cleaned_train_data_with_diacritics.txt', 'r', encoding='utf-8') as file:\n",
        "    training_data_with_diacritics_lines = file.readlines()\n",
        "    for i in range(len(training_data_with_diacritics_lines)):\n",
        "        training_data_with_diacritics_lines[i] = re.compile(r'[\\n\\r\\t]').sub('', training_data_with_diacritics_lines[i])\n",
        "        training_data_with_diacritics_lines[i] = re.compile(r'\\s+').sub(' ', training_data_with_diacritics_lines[i])\n",
        "        training_data_with_diacritics_lines[i] = training_data_with_diacritics_lines[i].strip()\n",
        "\n",
        "        remaining = training_data_with_diacritics_lines[i]\n",
        "        remaining_length = len(remaining)\n",
        "        while(remaining_length > 0):\n",
        "            spaces_to_include = training_spaces[spaces_index]\n",
        "            spaces_index += 1\n",
        "            words = remaining.split()\n",
        "            if len(words) <= spaces_to_include + 1:\n",
        "                training_data_with_diacritics.append(remaining.strip())\n",
        "                remaining_length = 0\n",
        "                break\n",
        "            else:\n",
        "                sentence = ' '.join(words[:spaces_to_include + 1])\n",
        "                training_data_with_diacritics.append(sentence.strip())\n",
        "                remaining = ' '.join(words[spaces_to_include + 1:]).strip()\n",
        "                remaining_length = len(remaining)\n",
        "\n",
        "validation_data_with_diacritics = []\n",
        "spaces_index = 0\n",
        "\n",
        "with open(f'{dataset_path}/cleaned_val_data_with_diacritics.txt', 'r', encoding='utf-8') as file:\n",
        "    validation_data_with_diacritics_lines = file.readlines()\n",
        "    for i in range(len(validation_data_with_diacritics_lines)):\n",
        "        validation_data_with_diacritics_lines[i] = re.compile(r'[\\n\\r\\t]').sub('', validation_data_with_diacritics_lines[i])\n",
        "        validation_data_with_diacritics_lines[i] = re.compile(r'\\s+').sub(' ', validation_data_with_diacritics_lines[i])\n",
        "        validation_data_with_diacritics_lines[i] = validation_data_with_diacritics_lines[i].strip()\n",
        "\n",
        "\n",
        "        remaining = validation_data_with_diacritics_lines[i]\n",
        "        remaining_length = len(remaining)\n",
        "        while(remaining_length > 0):\n",
        "            spaces_to_include = validation_spaces[spaces_index]\n",
        "            spaces_index += 1\n",
        "            words = remaining.split()\n",
        "            if len(words) <= spaces_to_include + 1:\n",
        "                validation_data_with_diacritics.append(remaining.strip())\n",
        "                remaining_length = 0\n",
        "                break\n",
        "            else:\n",
        "                sentence = ' '.join(words[:spaces_to_include + 1])\n",
        "                validation_data_with_diacritics.append(sentence.strip())\n",
        "                remaining = ' '.join(words[spaces_to_include + 1:]).strip()\n",
        "                remaining_length = len(remaining)\n",
        "\n",
        "# test equality\n",
        "for i in range(len(training_data)):\n",
        "    if(training_data[i] != remove_diactrics([training_data_with_diacritics[i]])[0])  or len(training_data[i]) > max_len:\n",
        "        print('Diacritized text != cleaned text!')\n",
        "\n",
        "for i in range(len(validation_data)):\n",
        "    if validation_data[i] != remove_diactrics([validation_data_with_diacritics[i]])[0] or len(validation_data[i]) > max_len :\n",
        "        print('Diacritized text != cleaned text!')\n",
        "\n",
        "print(len(training_data))\n",
        "print(len(validation_data))\n",
        "print(len(training_data_with_diacritics))\n",
        "print(len(validation_data_with_diacritics))\n",
        "\n",
        "print(training_data[0])\n",
        "print(training_data_with_diacritics[0])\n",
        "\n",
        "#vocab = set(''.join(training_data + validation_data))\n",
        "\n",
        "## Tokenize the text into sequences at the character level\n",
        "#char_to_index = {char: idx + 1 for idx, char in enumerate(vocab)}\n",
        "#index_to_char = {idx + 1: char for idx, char in enumerate(vocab)}\n",
        "\n",
        "char_to_index = {'د': 1, '؟': 2, 'آ': 3, 'إ': 4, 'ؤ': 5, 'ط': 6, 'م': 7, '،': 8, 'ة': 9, 'ت': 10, 'ر': 11, 'ئ': 12, 'ا': 13, 'ض': 14, '!': 15, ' ': 16, 'ك': 17, 'غ': 18, 'س': 19, 'ص': 20, 'أ': 21, 'ل': 22, 'ف': 23, 'ظ': 24, 'ج': 25, '؛': 26, 'ن': 27, 'ع': 28, 'ب': 29, 'ث': 30, 'ه': 31, 'خ': 32, 'ى': 33, 'ء': 34, 'ز': 35, 'ق': 36, 'ي': 37, 'ش': 38, 'ح': 39, ':': 40, 'ذ': 41, 'و': 42, '.': 43}\n",
        "index_to_char = {1: 'د', 2: '؟', 3: 'آ', 4: 'إ', 5: 'ؤ', 6: 'ط', 7: 'م', 8: '،', 9: 'ة', 10: 'ت', 11: 'ر', 12: 'ئ', 13: 'ا', 14: 'ض', 15: '!', 16: ' ', 17: 'ك', 18: 'غ', 19: 'س', 20: 'ص', 21: 'أ', 22: 'ل', 23: 'ف', 24: 'ظ', 25: 'ج', 26: '؛', 27: 'ن', 28: 'ع', 29: 'ب', 30: 'ث', 31: 'ه', 32: 'خ', 33: 'ى', 34: 'ء', 35: 'ز', 36: 'ق', 37: 'ي', 38: 'ش', 39: 'ح', 40: ':', 41: 'ذ', 42: 'و', 43: '.'}\n",
        "#char_to_index = {'د': 1, 'آ': 2, 'إ': 3, 'ؤ': 4, 'ط': 5, 'م': 6, 'ة': 7, 'ت': 8, 'ر': 9, 'ئ': 10, 'ا': 11, 'ض': 12, '!': 13, ' ': 14, 'ك': 15, 'غ': 16, 'س': 17, 'ص': 18, 'أ': 19, 'ل': 20, 'ف': 21, 'ظ': 22, 'ج': 23, 'ن': 24, 'ع': 25, 'ب': 26, 'ث': 27, 'ه': 28, 'خ': 29, 'ى': 30, 'ء': 31, 'ز': 32, 'ق': 33, 'ي': 34, 'ش': 35, 'ح': 36, 'ذ': 37, 'و': 38, '.': 39}\n",
        "#index_to_char = {1: 'د', 2: 'آ', 3: 'إ', 4: 'ؤ', 5: 'ط', 6: 'م', 7: 'ة', 8: 'ت', 9: 'ر', 10: 'ئ', 11: 'ا', 12: 'ض', 13: '!', 14: ' ', 15: 'ك', 16: 'غ', 17: 'س', 18: 'ص', 19: 'أ', 20: 'ل', 21: 'ف', 22: 'ظ', 23: 'ج', 24: 'ن', 25: 'ع', 26: 'ب', 27: 'ث', 28: 'ه', 29: 'خ', 30: 'ى', 31: 'ء', 32: 'ز', 33: 'ق', 34: 'ي', 35: 'ش', 36: 'ح', 37: 'ذ', 38: 'و', 39: '.'}\n",
        "print(char_to_index)\n",
        "print(index_to_char)\n",
        "\n",
        "#for char, index in char_to_index.items():\n",
        "#    print(f'{char} => {index}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7BAY3qGPj8v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85709fb3-e653-4240-a129-8183acdc2a1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'د': 1, '؟': 2, 'آ': 3, 'إ': 4, 'ؤ': 5, 'ط': 6, 'م': 7, '،': 8, 'ة': 9, 'ت': 10, 'ر': 11, 'ئ': 12, 'ا': 13, 'ض': 14, '!': 15, ' ': 16, 'ك': 17, 'غ': 18, 'س': 19, 'ص': 20, 'أ': 21, 'ل': 22, 'ف': 23, 'ظ': 24, 'ج': 25, '؛': 26, 'ن': 27, 'ع': 28, 'ب': 29, 'ث': 30, 'ه': 31, 'خ': 32, 'ى': 33, 'ء': 34, 'ز': 35, 'ق': 36, 'ي': 37, 'ش': 38, 'ح': 39, ':': 40, 'ذ': 41, 'و': 42, '.': 43}\n",
            "{1: 'د', 2: '؟', 3: 'آ', 4: 'إ', 5: 'ؤ', 6: 'ط', 7: 'م', 8: '،', 9: 'ة', 10: 'ت', 11: 'ر', 12: 'ئ', 13: 'ا', 14: 'ض', 15: '!', 16: ' ', 17: 'ك', 18: 'غ', 19: 'س', 20: 'ص', 21: 'أ', 22: 'ل', 23: 'ف', 24: 'ظ', 25: 'ج', 26: '؛', 27: 'ن', 28: 'ع', 29: 'ب', 30: 'ث', 31: 'ه', 32: 'خ', 33: 'ى', 34: 'ء', 35: 'ز', 36: 'ق', 37: 'ي', 38: 'ش', 39: 'ح', 40: ':', 41: 'ذ', 42: 'و', 43: '.'}\n"
          ]
        }
      ],
      "source": [
        "print(char_to_index)\n",
        "print(index_to_char)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSc7wyxVbMeI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d1418e9-1e4a-4860-eb57-8b19f711876b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1614: 0, 1611: 1, 1615: 2, 1612: 3, 1616: 4, 1613: 5, 1618: 6, 1617: 7, (1617, 1614): 8, (1617, 1611): 9, (1617, 1615): 10, (1617, 1612): 11, (1617, 1616): 12, (1617, 1613): 13, 0: 14, 15: 15}\n",
            "{0: 1614, 1: 1611, 2: 1615, 3: 1612, 4: 1616, 5: 1613, 6: 1618, 7: 1617, 8: (1617, 1614), 9: (1617, 1611), 10: (1617, 1615), 11: (1617, 1612), 12: (1617, 1616), 13: (1617, 1613), 14: 0, 15: 15}\n"
          ]
        }
      ],
      "source": [
        "# define the diacritics unicode and their corresponding labels classes indices\n",
        "# note that index 14 is reserved for no diacritic\n",
        "labels = {\n",
        "    # fath\n",
        "    1614: 0,\n",
        "    # tanween bel fath\n",
        "    1611: 1,\n",
        "    # damm\n",
        "    1615: 2,\n",
        "    # tanween bel damm\n",
        "    1612: 3,\n",
        "    # kasr\n",
        "    1616: 4,\n",
        "    # tanween bel kasr\n",
        "    1613: 5,\n",
        "    # sukun\n",
        "    1618: 6,\n",
        "    # shadd\n",
        "    1617: 7,\n",
        "    # shadd and fath\n",
        "    (1617, 1614): 8,\n",
        "    # shadd and tanween bel fath\n",
        "    (1617, 1611): 9,\n",
        "    # shadd and damm\n",
        "    (1617, 1615): 10,\n",
        "    # shadd and tanween bel damm\n",
        "    (1617, 1612): 11,\n",
        "    # shadd and kasr\n",
        "    (1617, 1616): 12,\n",
        "    # shadd and tanween bel kasr\n",
        "    (1617, 1613): 13,\n",
        "    # no diacritic\n",
        "    0: 14,\n",
        "    # padded\n",
        "    15: 15\n",
        "}\n",
        "\n",
        "indicies_to_labels = {\n",
        "    # fath\n",
        "    0: 1614,\n",
        "    # tanween bel fath\n",
        "    1: 1611,\n",
        "    # damm\n",
        "    2: 1615,\n",
        "    # tanween bel damm\n",
        "    3: 1612,\n",
        "    # kasr\n",
        "    4: 1616,\n",
        "    # tanween bel kasr\n",
        "    5: 1613,\n",
        "    # sukun\n",
        "    6: 1618,\n",
        "    # shadd\n",
        "    7: 1617,\n",
        "    # shadd and fath\n",
        "    8: (1617, 1614),\n",
        "    # shadd and tanween bel fath\n",
        "    9: (1617, 1611),\n",
        "    # shadd and damm\n",
        "    10: (1617, 1615),\n",
        "    # shadd and tanween bel damm\n",
        "    11: (1617, 1612),\n",
        "    # shadd and kasr\n",
        "    12: (1617, 1616),\n",
        "    # shadd and tanween bel kasr\n",
        "    13: (1617, 1613),\n",
        "    # no diacritic\n",
        "    14: 0,\n",
        "    # padded\n",
        "    15: 15\n",
        "}\n",
        "\n",
        "print(labels)\n",
        "print(indicies_to_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OC0kZjOMchbT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "547dec82-c799-4452-fa02-59fc110a7b89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"); print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLAlg5w_cW7D"
      },
      "outputs": [],
      "source": [
        "# build one array that holds all sequences of training data\n",
        "training_data_sequences = [[char_to_index[char] for char in sequence] for sequence in training_data]\n",
        "\n",
        "# build one array that holds all sequences of validation data\n",
        "validation_data_sequences = [[char_to_index[char] for char in sequence] for sequence in validation_data]\n",
        "\n",
        "# Pad sequences to the maximum length\n",
        "training_data_sequences = [sequence + [0] * (max_len - len(sequence)) for sequence in training_data_sequences]\n",
        "\n",
        "# Pad sequences to the maximum length\n",
        "validation_data_sequences = [sequence + [0] * (max_len - len(sequence)) for sequence in validation_data_sequences]\n",
        "\n",
        "training_data_sequences = torch.tensor(training_data_sequences).to(device)\n",
        "\n",
        "validation_data_sequences = torch.tensor(validation_data_sequences).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLf3zhWxcaqL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57bf98ac-9c67-49b5-8a80-de55c9e745b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "56366\n",
            "[0, 6, 2, 2, 15, 15, 15, 0, 6, 15]\n",
            "قوله : أو \n",
            "33819600\n",
            "33819600\n"
          ]
        }
      ],
      "source": [
        "training_data_labels = []\n",
        "training_size = len(training_data_with_diacritics)\n",
        "for sentence_index in range(training_size):\n",
        "  sentence_labels = []\n",
        "  sentence_size = len(training_data_with_diacritics[sentence_index])\n",
        "  index = 0\n",
        "  while index < sentence_size:\n",
        "      if ord(training_data_with_diacritics[sentence_index][index]) not in labels:\n",
        "          char_sequence = char_to_index[training_data_with_diacritics[sentence_index][index]]\n",
        "          if char_sequence == 2 or char_sequence == 8 or char_sequence == 15 or char_sequence == 16 or char_sequence == 26 or char_sequence == 40 or char_sequence == 43:\n",
        "              # unwanted char\n",
        "              sentence_labels.append(15)\n",
        "              index += 1\n",
        "              continue\n",
        "          # char is not a diacritic\n",
        "          if (index + 1) < sentence_size and ord(training_data_with_diacritics[sentence_index][index + 1]) in labels:\n",
        "              # char has a diacritic\n",
        "              if ord(training_data_with_diacritics[sentence_index][index + 1]) == 1617:\n",
        "                  # char has a shadd diacritic\n",
        "                  if (index + 2) < sentence_size and ord(training_data_with_diacritics[sentence_index][index + 2]) in labels:\n",
        "                      # char has a shadd and another diacritic\n",
        "                      sentence_labels.append(labels[(1617, ord(training_data_with_diacritics[sentence_index][index + 2]))])\n",
        "                      # skip next 2 diacritics chars\n",
        "                      index += 3  # increment by 3 to skip two diacritic chars\n",
        "                      continue\n",
        "                  else:\n",
        "                      # char has a shadd and no other diacritic\n",
        "                      sentence_labels.append(labels[1617])\n",
        "                      # skip next diacritic char\n",
        "                      index += 2\n",
        "                      continue\n",
        "              # char has a diacritic other than shadd\n",
        "              sentence_labels.append(labels[ord(training_data_with_diacritics[sentence_index][index + 1])])\n",
        "              # skip next diacritic char\n",
        "              index += 2  # increment by 2 to skip one diacritic char\n",
        "              continue\n",
        "          else:\n",
        "              # char has no diacritic\n",
        "              sentence_labels.append(14)\n",
        "      index += 1  # increment by 1 for normal iteration\n",
        "\n",
        "  training_data_labels.append(sentence_labels)\n",
        "\n",
        "print(len(training_data_labels))\n",
        "print(training_data_labels[0][:10])\n",
        "print(training_data[0][:10])\n",
        "\n",
        "# Pad sequences to the maximum length\n",
        "training_data_labels = [sequence + [15] * (max_len - len(sequence)) for sequence in training_data_labels]\n",
        "\n",
        "print(len(training_data_labels) * len(training_data_labels[0]))\n",
        "print(len(training_data_sequences) * len(training_data_sequences[0]))\n",
        "\n",
        "training_data_labels = torch.tensor(training_data_labels).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNVh1NDtcy-4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52c05a86-3858-4280-c7f3-a1c014337f83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2696\n",
            "[0, 6, 2, 2, 15, 15, 15, 0, 0, 14]\n",
            "قوله : ولا\n",
            "1617600\n",
            "1617600\n"
          ]
        }
      ],
      "source": [
        "validation_data_labels = []\n",
        "validation_size = len(validation_data_with_diacritics)\n",
        "for sentence_index in range(validation_size):\n",
        "  sentence_labels = []\n",
        "  sentence_size = len(validation_data_with_diacritics[sentence_index])\n",
        "  index = 0\n",
        "  while index < sentence_size:\n",
        "      if ord(validation_data_with_diacritics[sentence_index][index]) not in labels:\n",
        "          char_sequence = char_to_index[validation_data_with_diacritics[sentence_index][index]]\n",
        "          if char_sequence == 2 or char_sequence == 8 or char_sequence == 15 or char_sequence == 16 or char_sequence == 26 or char_sequence == 40 or char_sequence == 43:\n",
        "              # unwanted char\n",
        "              sentence_labels.append(15)\n",
        "              index += 1\n",
        "              continue\n",
        "          # char is not a diacritic\n",
        "          if (index + 1) < sentence_size and ord(validation_data_with_diacritics[sentence_index][index + 1]) in labels:\n",
        "              # char has a diacritic\n",
        "              if ord(validation_data_with_diacritics[sentence_index][index + 1]) == 1617:\n",
        "                  # char has a shadd diacritic\n",
        "                  if (index + 2) < sentence_size and ord(validation_data_with_diacritics[sentence_index][index + 2]) in labels:\n",
        "                      # char has a shadd and another diacritic\n",
        "                      sentence_labels.append(labels[(1617, ord(validation_data_with_diacritics[sentence_index][index + 2]))])\n",
        "                      # skip next 2 diacritics chars\n",
        "                      index += 3  # increment by 3 to skip two diacritic chars\n",
        "                      continue\n",
        "                  else:\n",
        "                      # char has a shadd and no other diacritic\n",
        "                      sentence_labels.append(labels[1617])\n",
        "                      # skip next diacritic char\n",
        "                      index += 2\n",
        "                      continue\n",
        "              # char has a diacritic other than shadd\n",
        "              sentence_labels.append(labels[ord(validation_data_with_diacritics[sentence_index][index + 1])])\n",
        "              # skip next diacritic char\n",
        "              index += 2  # increment by 2 to skip one diacritic char\n",
        "              continue\n",
        "          else:\n",
        "              # char has no diacritic\n",
        "              sentence_labels.append(14)\n",
        "      index += 1  # increment by 1 for normal iteration\n",
        "\n",
        "  validation_data_labels.append(sentence_labels)\n",
        "\n",
        "print(len(validation_data_labels))\n",
        "print(validation_data_labels[0][:10])\n",
        "print(validation_data[0][:10])\n",
        "\n",
        "# Pad sequences to the maximum length\n",
        "validation_data_labels = [sequence + [15] * (max_len - len(sequence)) for sequence in validation_data_labels]\n",
        "\n",
        "print(len(validation_data_labels) * len(validation_data_labels[0]))\n",
        "print(len(validation_data_sequences) * len(validation_data_sequences[0]))\n",
        "\n",
        "validation_data_labels = torch.tensor(validation_data_labels).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SytzHHIodEEP"
      },
      "outputs": [],
      "source": [
        "training_dataset = TensorDataset(training_data_sequences, training_data_labels)\n",
        "\n",
        "training_batch_size = 32\n",
        "training_dataloader = DataLoader(training_dataset, batch_size=training_batch_size)\n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "validation_dataset = TensorDataset(validation_data_sequences, validation_data_labels)\n",
        "\n",
        "validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4JwjBENdQ4_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9305c4b1-81bf-46a6-8a40-d14dd2e6530f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CharLSTM(\n",
            "  (embedding): Embedding(44, 300)\n",
            "  (lstm): LSTM(300, 256, num_layers=3, batch_first=True, dropout=0.1, bidirectional=True)\n",
            "  (batchnorm): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (output): Linear(in_features=512, out_features=16, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "from torch.nn import BatchNorm1d\n",
        "\n",
        "prev_layer_size = 600\n",
        "\n",
        "class CharLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size, output_size, num_layers=1):\n",
        "        super(CharLSTM, self).__init__()\n",
        "        # chars embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "\n",
        "        # LSTM layers\n",
        "        # batch_first: it means that the input tensor has its first dimension representing the batch size\n",
        "        self.lstm = nn.LSTM(input_size=embedding_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True, dropout=dropout_rate)\n",
        "\n",
        "        self.batchnorm = BatchNorm1d(prev_layer_size)  # Add BatchNorm layer\n",
        "\n",
        "        self.output = nn.Linear(hidden_size * 2, output_size)\n",
        "\n",
        "        # output layer\n",
        "        self.output = nn.Linear(hidden_size * 2, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x) # batch_size * seq_length * embedding_size\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "        lstm_out = self.batchnorm(lstm_out)  # Apply BatchNorm\n",
        "        output = self.output(lstm_out)\n",
        "        return output\n",
        "\n",
        "num_layers = 3\n",
        "vocab_size = len(char_to_index) + 1 # +1 for the 0 padding\n",
        "embedding_size = 300\n",
        "dropout_rate = 0.1\n",
        "output_size = len(labels)\n",
        "hidden_size = 256\n",
        "lr=0.001\n",
        "num_epochs = 10\n",
        "lr_step_size = 5\n",
        "lr_gamma = 0.1\n",
        "\n",
        "\n",
        "model = CharLSTM(vocab_size, embedding_size, hidden_size, output_size, num_layers).to(device)\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLE7O9P4dXky",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39a9a5fa-4ffb-442b-dcb2-7e736d61aee8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Training Loss: 0.0537786, Validation Accuracy: 99.238%\n",
            "Epoch 2/10, Training Loss: 0.0513979, Validation Accuracy: 99.351%\n",
            "Epoch 3/10, Training Loss: 0.0477198, Validation Accuracy: 99.446%\n"
          ]
        }
      ],
      "source": [
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=15)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-08)\n",
        "scheduler = StepLR(optimizer, step_size=lr_step_size, gamma=lr_gamma)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for batch_sequences, batch_labels in training_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_sequences)\n",
        "\n",
        "        flat_outputs = outputs.view(-1, outputs.shape[-1])\n",
        "        flat_labels = batch_labels.view(-1)\n",
        "\n",
        "        mask = (flat_labels != 15)\n",
        "        loss = criterion(flat_outputs[mask], flat_labels[mask])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    average_loss = total_loss / len(training_dataloader)\n",
        "\n",
        "    model.eval()\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for validation_batch_sequences, validation_batch_labels in validation_dataloader:\n",
        "            outputs = model(validation_batch_sequences)\n",
        "            predicted_labels = outputs.argmax(dim=2)\n",
        "\n",
        "            mask = (validation_batch_labels != 15) & (validation_batch_sequences != 2) & (validation_batch_sequences != 8) & (validation_batch_sequences != 15) & (validation_batch_sequences != 16) & (validation_batch_sequences != 26) & (validation_batch_sequences != 40) & (validation_batch_sequences != 43)\n",
        "            correct_predictions += ((predicted_labels == validation_batch_labels) & mask).sum().item()\n",
        "            total_predictions += mask.sum().item()\n",
        "\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Training Loss: {average_loss:.7f}, Validation Accuracy: {accuracy * 100:.3f}%')\n",
        "\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kL-JJh7M36ZT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "d5de0a43-dbe2-416f-f322-5806f250f42b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-3e1704a2e02e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# save the model in pkl file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"/content/Arabic-Text-Diacritization/dataset/BiLSTM_Loss=_Accuracy=last%_embedding_size={embedding_size}hidden_size={hidden_size}lr={lr}num_layers={num_layers}num_epochs={num_epochs}max_len={max_len}batch_size={training_batch_size}_lr_step_size={lr_step_size}_lr_gamma={lr_gamma}_dropout_rate={dropout_rate}.pkl\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'embedding_size' is not defined"
          ]
        }
      ],
      "source": [
        "# save the model in pkl file\n",
        "file_path = f\"/content/Arabic-Text-Diacritization/dataset/BiLSTM_Loss={last_loss * 1:.7f}_Accuracy={accuracy * 100:.3f}%_embedding_size={embedding_size}hidden_size={hidden_size}lr={lr}num_layers={num_layers}num_epochs={num_epochs}max_len={max_len}batch_size={training_batch_size}_lr_step_size={lr_step_size}_lr_gamma={lr_gamma}_dropout_rate={dropout_rate}.pkl\"\n",
        "with open(file_path, 'wb') as file:\n",
        "    pickle.dump(model, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvtumWLzeExI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "5c54fd18-6d34-417a-f5f6-94c831272dd3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/NLP/BiLSTM_Loss=_Accuracy=99.365%_embedding_size=300hidden_size=256lr=0.001num_layers=3num_epochs=10max_len=600batch_size=32_lr_step_size=5_lr_gamma=0.1_dropout_rate=0.1.pkl'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "import shutil\n",
        "shutil.copy(file_path,\"/content/drive/MyDrive/NLP\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E39hDfcts9qc"
      },
      "outputs": [],
      "source": [
        "def lstm_predict(model, sentence):\n",
        "    global device\n",
        "    global max_len\n",
        "    global char_to_index\n",
        "    global indicies_to_labels\n",
        "    sentence_sequences = [[char_to_index[char] for char in sentence]]\n",
        "\n",
        "    # Pad sequences to the maximum length\n",
        "    sentence_sequences[0] = sentence_sequences[0] + [0] * (max_len - len(sentence_sequences[0]))\n",
        "\n",
        "    sentence_sequences = torch.tensor(sentence_sequences).view(1, -1).to(device)  # Assuming batch size 1\n",
        "\n",
        "    print(sentence_sequences.shape)\n",
        "    outputs = model(sentence_sequences)\n",
        "    print(outputs.shape)\n",
        "    outputs = outputs.argmax(dim=2)\n",
        "    print(outputs.shape)\n",
        "    outputs = outputs.tolist()\n",
        "    print(outputs)\n",
        "    diacritics = []\n",
        "    for output in outputs:\n",
        "        for index in output:\n",
        "            predicted_class = indicies_to_labels[index]\n",
        "            if type(predicted_class) is tuple:\n",
        "                diacritics.append(chr(predicted_class[0]) + chr(predicted_class[1]))\n",
        "            elif predicted_class == 0:\n",
        "                diacritics.append('')\n",
        "            else:\n",
        "                diacritics.append(chr(predicted_class))\n",
        "    print(diacritics)\n",
        "    return diacritics[:len(sentence)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1f17Q53s_YM"
      },
      "outputs": [],
      "source": [
        "test_sentence = 'قوله تعالى بإذن أهلهن'\n",
        "test_sentence = re.compile(r'[\\n+\\r+\\t+]').sub('', test_sentence)\n",
        "predicted_diacritics = lstm_predict(model, test_sentence)\n",
        "\n",
        "diacritized_sentence = ''\n",
        "for i in range(len(test_sentence)):\n",
        "    diacritized_sentence += test_sentence[i] + predicted_diacritics[i]\n",
        "\n",
        "print(diacritized_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "on7qXFdy8w39"
      },
      "outputs": [],
      "source": [
        "file_name = \"BiLSTM_Loss=0.0162523_Accuracy=96.598%_embedding_size=200hidden_size=256lr=0.001num_layers=4num_epochs=13max_len=100.pkl\"\n",
        "!gdown \"1-50Bs85dmpml3rq6wDrFFr7THrowv9fn\" -O \"BiLSTM_Loss=0.0162523_Accuracy=96.598%_embedding_size=200hidden_size=256lr=0.001num_layers=4num_epochs=13max_len=100.pkl\"\n",
        "with open(file_name, \"rb\") as file:\n",
        "  model = pickle.load(file)\n",
        "\n",
        "# test model\n",
        "correct_predictions = 0\n",
        "total_predictions = 0\n",
        "model.eval()\n",
        "for validation_batch_sequences, validation_batch_labels in validation_dataloader:\n",
        "    outputs = model(validation_batch_sequences) # batch_size * seq_length * output_size\n",
        "    # Calculate accuracy\n",
        "    predicted_labels = outputs.argmax(dim=2)  # Get the index with the maximum probability\n",
        "    mask = (validation_batch_labels != 15) #& (validation_batch_sequences != 2) & (validation_batch_sequences != 8) & (validation_batch_sequences != 16) & (validation_batch_sequences != 26) & (validation_batch_sequences != 40)\n",
        "    #mask = (validation_batch_labels != 15) & (validation_batch_sequences != 14)\n",
        "    correct_predictions += ((predicted_labels == validation_batch_labels) & mask).sum().item()\n",
        "    total_predictions += mask.sum().item()\n",
        "\n",
        "accuracy = correct_predictions / total_predictions\n",
        "\n",
        "print(f'After reading, Accuracy: {accuracy * 100:.3f}%')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}