{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_pattern(text,pattern,replace = ''):\n",
    "    # Replace diacritics from the text\n",
    "    cleaned_text = pattern.sub(replace, text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "arabic_text = \"( وَلَيْسَ لَهُ وَضْعُ خَشَبَةٍ عَلَى حَائِطِ جَارِهِ أَوْ ) الْحَائِطِ ( الْمُشْتَرَكِ ) بِلَا إذْنِهِ ( إلَّا عِنْدَ الضَّرُورَةِ بِأَنْ لَا يُمْكِنَهُ التَّسْقِيفُ إلَّا بِهِ ) أَيْ بِوَضْعِ الْخَشَبِ عَلَى حَائِطِ الْجَارِ أَوْ الْمُشْتَرَكِ ( فَيَجُوزُ ) وَضْعُهُ ، سَوَاءٌ كَانَ لَهُ حَائِطٌ وَاحِدٌ أَوْ حَائِطَانِ لِحَدِيثِ أَبُو هُرَيْرَةَ مَرْفُوعًا { لَا يَمْنَعَنَّ جَارٌ جَارَهُ أَنْ يَضَعَ خَشَبَةً عَلَى جِدَارِهِ ، ثُمَّ يَقُولُ أَبُو هُرَيْرَةَ مَا لِي أَرَاكُمْ عَنْهَا مُعْرِضِينَ وَاَللَّهِ لَأَرْمِيَنَّ بِهَا بَيْنَ أَكْتَافِكُمْ } مُتَّفَقٌ عَلَيْهِ وَمَعْنَاهُ : لَأَضَعَنَّ هَذِهِ السُّنَّةَ بَيْنَ أَكْتَافِكُمْ ، وَلِأَحْمِلَنكُمْ عَلَى الْعَمَلِ بِهَا وَقِيلَ مَعْنَاهُ : لَأَضَعَنَّ جُذُوعَ الْجِيرَانِ عَلَى أَكْتَافِكُمْ مُبَالَغَةً وَلِأَنَّهُ انْتِفَاعٌ بِحَائِطِ جَارِهِ عَلَى وَجْهٍ لَا يَضُرُّ بِهِ أَشْبَهَ الِاسْتِنَادَ إلَيْهِ وَإِنْ أَمْكَنَ وَضْعُهُ عَلَى غَيْرِهِ لَمْ يَجُزْ وَضْعُهُ عَلَيْهِ إلَّا بِإِذْنِ رَبِّهِ وَإِنْ لَمْ يُمْكِنْ إلَّا بِهِ جَازَ ( وَلَوْ ) كَانَ الْحَائِطُ ( لِيَتِيمٍ وَمَجْنُونٍ ) أَوْ مُكَاتَبٍ أَوْ وَقْفٍ وَنَحْوِهِ ، لِعُمُومِ مَا سَبَقَ ( مَا لَمْ يَتَضَرَّرْ الْحَائِطُ ) بِوَضْعِ الْخَشَبِ عَلَيْهِ .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    # remove any brackets that have only numbers inside and remove all numbers \n",
    "    reg = r'\\(\\s*(\\d+)\\s*\\/\\s*(\\d+)\\s*\\)|\\d+'\n",
    "    text = replace_pattern(text, re.compile(reg))\n",
    "    # replace all different types of brackets with a single type\n",
    "    reg_opening_brackets = r'[\\[\\{]'\n",
    "    reg_closing_brackets = r'[\\]\\}]'\n",
    "    text = replace_pattern(text, re.compile(reg_opening_brackets), '(')\n",
    "    text = replace_pattern(text, re.compile(reg_closing_brackets), ')')\n",
    "    # remove some unwanted characters\n",
    "    reg = r'[/!\\-؛،؟:\\.]'\n",
    "    text = replace_pattern(text, re.compile(reg))\n",
    "    # remove extra spaces\n",
    "    reg = r'\\s+'\n",
    "    text = replace_pattern(text, re.compile(reg), ' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_words_between_brackets(text):\n",
    "    # Define a regular expression pattern to match words between brackets\n",
    "    pattern_in_brackets = re.compile(r'\\((.*?)\\)')\n",
    "    # Find all matches in the text\n",
    "    matches_in_brackets = pattern_in_brackets.findall(text)\n",
    "    # Join all matches into a single string to form a sentence\n",
    "    matches_in_brackets = [match.strip() for match in matches_in_brackets]\n",
    "\n",
    "    # Define a regular expression pattern to match sentences outside brackets\n",
    "    pattern_outside_brackets = re.compile(r'[^()]+(?=\\()|(?<=\\))[^()]+')\n",
    "    # Find all matches in the text\n",
    "    matches_outside_brackets = pattern_outside_brackets.findall(text)\n",
    "    matches_outside_brackets = [match.strip() for match in matches_outside_brackets]\n",
    "    matches_in_brackets.extend(matches_outside_brackets)\n",
    "    return matches_in_brackets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_diactrics(text):\n",
    "    # remove diacritics\n",
    "    reg = r'[\\u064B-\\u065F\\u0670\\uFE70-\\uFE7F]'\n",
    "    return replace_pattern(text, re.compile(reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    # clean the text from unwanted characters\n",
    "    text = clean(text)\n",
    "    # split the text into sentences\n",
    "    text = split_words_between_brackets(text)\n",
    "    # save the cleaned text with diacritics to a file \n",
    "    with open('dataset/cleaned_train_data_with_diacritics.txt', 'a+',encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(text))\n",
    "    # remove diacritics\n",
    "    text = [remove_diactrics(sentence) for sentence in text]\n",
    "    # save the cleaned text without diacritics to a file\n",
    "    with open('dataset/cleaned_train_data_without_diacritics.txt', 'a+',encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(text))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # tokenize the text\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    # tokens that have list of sentences and each sentence is a list of words\n",
    "    sentences = [tokenizer.tokenize(sentence) for sentence in text]\n",
    "    filtered_sentences = []\n",
    "    for sentence in sentences:\n",
    "        filtered_tokens = [token for token in sentence if token not in stopwords.words('arabic')]\n",
    "        if filtered_tokens != []: filtered_sentences.append(filtered_tokens)\n",
    "    return filtered_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "# read the train data and clean it and save it to the files\n",
    "with open('dataset/train.txt', 'r',encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        sentences.extend(preprocess(line))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data from cleaned files\n",
    "with open('dataset/cleaned_train_data_with_diacritics.txt', 'r',encoding='utf-8') as f:\n",
    "    sentences_with_diacritics = f.readlines()\n",
    "with open('dataset/cleaned_train_data_without_diacritics.txt', 'r',encoding='utf-8') as f:\n",
    "    sentences_without_diacritics = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the data\n",
    "sentences_without_diacritics = tokenize(sentences_without_diacritics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
