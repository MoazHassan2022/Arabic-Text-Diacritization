{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath('../..'))\n",
    "from preprocessing.preprocess import preprocess_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5030669\n",
      "421099\n",
      "9419289\n",
      "{'ق': 1, 'ب': 2, 'ي': 3, 'ف': 4, 'أ': 5, 'و': 6, 'ر': 7, 'ط': 8, 'ة': 9, 'د': 10, 'س': 11, 'ك': 12, 'غ': 13, 'إ': 14, 'ت': 15, 'ج': 16, 'ه': 17, 'ز': 18, 'ص': 19, 'ذ': 20, 'ا': 21, 'ش': 22, 'خ': 23, 'ئ': 24, 'ء': 25, 'ؤ': 26, 'آ': 27, 'ن': 28, 'ى': 29, 'ظ': 30, 'ث': 31, 'م': 32, 'ض': 33, 'ع': 34, 'ح': 35, '~': 36, 'ل': 37}\n",
      "{1: 'ق', 2: 'ب', 3: 'ي', 4: 'ف', 5: 'أ', 6: 'و', 7: 'ر', 8: 'ط', 9: 'ة', 10: 'د', 11: 'س', 12: 'ك', 13: 'غ', 14: 'إ', 15: 'ت', 16: 'ج', 17: 'ه', 18: 'ز', 19: 'ص', 20: 'ذ', 21: 'ا', 22: 'ش', 23: 'خ', 24: 'ئ', 25: 'ء', 26: 'ؤ', 27: 'آ', 28: 'ن', 29: 'ى', 30: 'ظ', 31: 'ث', 32: 'م', 33: 'ض', 34: 'ع', 35: 'ح', 36: '~', 37: 'ل'}\n"
     ]
    }
   ],
   "source": [
    "# lines limit\n",
    "limit = 30000\n",
    "dataset_path = '../../dataset'\n",
    "\n",
    "# preprocess and save the data\n",
    "preprocess_data(data_type='train', limit=limit, dataset_path=dataset_path)\n",
    "preprocess_data(data_type='val', limit=limit, dataset_path=dataset_path)\n",
    "\n",
    "# load data\n",
    "with open(f'{dataset_path}/cleaned_train_data_without_diacritics.txt', 'r', encoding='utf-8') as file:\n",
    "    # read all lines into a single string\n",
    "    training_data = re.compile(r'[\\n\\r\\t\\s]').sub('', file.read())\n",
    "with open(f'{dataset_path}/cleaned_val_data_without_diacritics.txt', 'r', encoding='utf-8') as file:\n",
    "    # read all lines into a single string\n",
    "    validation_data = re.compile(r'[\\n\\r\\t\\s]').sub('', file.read())\n",
    "    \n",
    "with open(f'{dataset_path}/cleaned_train_data_with_diacritics.txt', 'r', encoding='utf-8') as file:\n",
    "    # read all lines into a single string\n",
    "    training_data_with_diacritics = re.compile(r'[\\n\\r\\t\\s]').sub('', file.read())\n",
    "\n",
    "print(len(training_data))\n",
    "print(len(validation_data))\n",
    "print(len(training_data_with_diacritics))\n",
    "# Tokenize the text into sequences at the character level\n",
    "vocab = set(''.join(training_data + validation_data))\n",
    "\n",
    "char_to_index = {char: idx + 1 for idx, char in enumerate(vocab)}\n",
    "index_to_char = {idx + 1: char for idx, char in enumerate(vocab)}\n",
    "\n",
    "print(char_to_index)\n",
    "print(index_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"); print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1614: 1, 1615: 2, 1616: 3, 1617: 4, 1618: 5, 1611: 6, 1612: 7, 1613: 8, (1617, 1614): 9, (1617, 1615): 10, (1617, 1616): 11, (1617, 1611): 12, (1617, 1612): 13, (1617, 1613): 14}\n",
      "{0: 0, 1: 1614, 2: 1615, 3: 1616, 4: 1617, 5: 1618, 6: 1611, 7: 1612, 8: 1613, 9: (1617, 1614), 10: (1617, 1615), 11: (1617, 1616), 12: (1617, 1611), 13: (1617, 1612), 14: (1617, 1613)}\n"
     ]
    }
   ],
   "source": [
    "# define the diacritics unicode and their corresponding labels classes indices\n",
    "# note that index 0 is reserved for no diacritic\n",
    "labels = {\n",
    "    # no diacritic\n",
    "    0: 0,\n",
    "    # fath\n",
    "    1614: 1,\n",
    "    # damm\n",
    "    1615: 2,\n",
    "    # kasr\n",
    "    1616: 3,\n",
    "    # shadd\n",
    "    1617: 4,\n",
    "    # sukun\n",
    "    1618: 5,\n",
    "    # tanween bel fath\n",
    "    1611: 6,\n",
    "    # tanween bel damm\n",
    "    1612: 7,\n",
    "    # tanween bel kasr\n",
    "    1613: 8,\n",
    "    # shadd and fath\n",
    "    (1617, 1614): 9,\n",
    "    # shadd and damm\n",
    "    (1617, 1615): 10,\n",
    "    # shadd and kasr\n",
    "    (1617, 1616): 11,\n",
    "    # shadd and tanween bel fath\n",
    "    (1617, 1611): 12,\n",
    "    # shadd and tanween bel damm\n",
    "    (1617, 1612): 13,\n",
    "    # shadd and tanween bel kasr\n",
    "    (1617, 1613): 14\n",
    "}\n",
    "\n",
    "indicies_to_labels = {\n",
    "    # no diacritic\n",
    "    0: 0,\n",
    "    # fath\n",
    "    1: 1614,\n",
    "    # damm\n",
    "    2: 1615,\n",
    "    # kasr\n",
    "    3: 1616,\n",
    "    # shadd\n",
    "    4: 1617,\n",
    "    # sukun\n",
    "    5: 1618,\n",
    "    # tanween bel fath\n",
    "    6: 1611,\n",
    "    # tanween bel damm\n",
    "    7: 1612,\n",
    "    # tanween bel kasr\n",
    "    8: 1613,\n",
    "    # shadd and fath\n",
    "    9: (1617, 1614),\n",
    "    # shadd and damm\n",
    "    10: (1617, 1615),\n",
    "    # shadd and kasr\n",
    "    11: (1617, 1616),\n",
    "    # shadd and tanween bel fath\n",
    "    12: (1617, 1611),\n",
    "    # shadd and tanween bel damm\n",
    "    13: (1617, 1612),\n",
    "    # shadd and tanween bel kasr\n",
    "    14: (1617, 1613)\n",
    "}\n",
    "\n",
    "print(labels)\n",
    "print(indicies_to_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{dataset_path}/cleaned_train_data_with_diacritics.txt', 'r', encoding='utf-8') as f:\n",
    "    for i in range(10):\n",
    "        line = f.readline()\n",
    "        for char in line:\n",
    "            print(f'{char} : {ord(char)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# load data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdataset_path\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/cleaned_train_data_without_diacritics.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# read all lines into a single string\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     training_data \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mcompile(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, file\u001b[38;5;241m.\u001b[39mread())\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/cleaned_val_data_without_diacritics.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# read all lines into a single string\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset_path' is not defined"
     ]
    }
   ],
   "source": [
    "with open(f'{dataset_path}/cleaned_train_data_without_diacritics.txt', 'r', encoding='utf-8') as file:\n",
    "    training_data = file.readlines()\n",
    "    for i in range(len(training_data)):\n",
    "        training_data[i] = re.compile(r'[\\n+\\r+\\t+\\s+]').sub('', training_data[i])\n",
    "    \n",
    "with open(f'{dataset_path}/cleaned_train_data_with_diacritics.txt', 'r', encoding='utf-8') as file:\n",
    "    training_data_with_diacritics = file.readlines()\n",
    "    for i in range(len(training_data_with_diacritics)):\n",
    "        training_data_with_diacritics[i] = re.compile(r'[\\n+\\r+\\t+\\s+]').sub('', training_data_with_diacritics[i])\n",
    "\n",
    "print(len(training_data))\n",
    "print(len(training_data_with_diacritics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 6, 37, 17, 5, 6, 1, 8, 34, 21]\n",
      "5030669\n"
     ]
    }
   ],
   "source": [
    "# build one array that holds all sequences of training data\n",
    "training_data_sequences = [char_to_index[char] for char in training_data]\n",
    "print(training_data_sequences[:10])\n",
    "print(len(training_data_sequences))\n",
    "\n",
    "fixed_sequence_length = 50\n",
    "\n",
    "# Create fixed-length sequences\n",
    "fixed_sequences = [training_data_sequences[i:i+fixed_sequence_length] for i in range(0, len(training_data_sequences), fixed_sequence_length)]\n",
    "\n",
    "# Pad 0 to last sequence if it is less than fixed_sequence_length\n",
    "if len(fixed_sequences[-1]) < fixed_sequence_length:\n",
    "    fixed_sequences[-1] += [0] * (fixed_sequence_length - len(fixed_sequences[-1]))\n",
    "\n",
    "training_data_sequences = torch.tensor(fixed_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_with_diacritics = re.compile(r'[\\n+\\r+\\t+\\s+]').sub('', 'قَوْلُهُ أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَخْ قَالَ الزَّرْكَشِيُّ')\n",
    "print(training_data_with_diacritics)\n",
    "print(training_data_with_diacritics[-1])\n",
    "for char in training_data_with_diacritics:\n",
    "    print(f'{char} : {ord(char)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'و': 0, 'ء': 1, 'ى': 2, 'ض': 3, 'ث': 4, 'ج': 5, 'ة': 6, 'ؤ': 7, 'ق': 8, 'ن': 9, 'ف': 10, 'ذ': 11, 'ع': 12, 'د': 13, 'ز': 14, 'ب': 15, 'خ': 16, 'ي': 17, 'أ': 18, 'ت': 19, 'غ': 20, 'م': 21, 'ه': 22, 'ط': 23, 'ل': 24, 'ر': 25, 'س': 26, 'إ': 27, 'ك': 28, 'ش': 29, 'آ': 30, 'ص': 31, 'ئ': 32, 'ح': 33, 'ظ': 34, 'ا': 35}\n"
     ]
    }
   ],
   "source": [
    "print(char_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5030669\n",
      "[1, 5, 2, 2, 1, 5, 1, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "training_data_labels = []\n",
    "training_size = len(training_data_with_diacritics)\n",
    "index = 0\n",
    "while index < training_size:\n",
    "    if ord(training_data_with_diacritics[index]) not in labels:\n",
    "        # char is not a diacritic\n",
    "        if (index + 1) < training_size and ord(training_data_with_diacritics[index + 1]) in labels:\n",
    "            # char has a diacritic\n",
    "            if ord(training_data_with_diacritics[index + 1]) == 1617:\n",
    "                # char has a shadd diacritic\n",
    "                if (index + 2) < training_size and ord(training_data_with_diacritics[index + 2]) in labels:\n",
    "                    # char has a shadd and another diacritic\n",
    "                    training_data_labels.append(labels[(1617, ord(training_data_with_diacritics[index + 2]))])\n",
    "                    # skip next 2 diacritics chars\n",
    "                    index += 3  # increment by 3 to skip two diacritic chars\n",
    "                    continue\n",
    "                else:\n",
    "                    # char has a shadd and no other diacritic\n",
    "                    training_data_labels.append(labels[1617])\n",
    "                    # skip next diacritic char\n",
    "                    index += 2\n",
    "                    continue\n",
    "            # char has a diacritic other than shadd\n",
    "            training_data_labels.append(labels[ord(training_data_with_diacritics[index + 1])])\n",
    "            # skip next diacritic char\n",
    "            index += 2  # increment by 2 to skip one diacritic char\n",
    "            continue\n",
    "        else:\n",
    "            # char has no diacritic\n",
    "            training_data_labels.append(0)\n",
    "    index += 1  # increment by 1 for normal iteration\n",
    "\n",
    "print(len(training_data_labels))\n",
    "print(training_data_labels[:10])\n",
    "\n",
    "# Create fixed-length sequences\n",
    "fixed_labels = [training_data_labels[i:i+fixed_sequence_length] for i in range(0, len(training_data_labels), fixed_sequence_length)]\n",
    "\n",
    "# Pad 0 to last sequence if it is less than fixed_sequence_length\n",
    "if len(fixed_labels[-1]) < fixed_sequence_length:\n",
    "    fixed_labels[-1] += [0] * (fixed_sequence_length - len(fixed_labels[-1]))\n",
    "\n",
    "training_data_labels = torch.tensor(fixed_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(training_data_sequences, training_data_labels)\n",
    "\n",
    "batch_size = 32\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharLSTM(\n",
      "  (embedding): Embedding(38, 150)\n",
      "  (lstm): LSTM(150, 256, num_layers=2, batch_first=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (output): Linear(in_features=256, out_features=15, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, output_size, drop_prob=0.5, num_layers=1):\n",
    "        super(CharLSTM, self).__init__()\n",
    "        # chars embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        \n",
    "        # LSTM layers\n",
    "        # batch_first: it means that the input tensor has its first dimension representing the batch size\n",
    "        # TODO: BLSTM\n",
    "        self.lstm = nn.LSTM(input_size=embedding_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        \n",
    "        # Drop out layer, how likely would it drop some neurons (assign zeros to them)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        # output layer\n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x) # batch_size * seq_length * embedding_size\n",
    "        lstm_out, _ = self.lstm(embedded) # batch_size * seq_length * hidden_size\n",
    "        after_dropout = self.dropout(lstm_out) # batch_size * seq_length *  hidden_size\n",
    "        output = self.output(after_dropout)  # batch_size * seq_length * output_size\n",
    "        output_softmax = F.softmax(output, dim=1)  # Apply softmax to the output\n",
    "        return output_softmax\n",
    "    \n",
    "num_layers = 2\n",
    "vocab_size = len(char_to_index) + 1 # +1 for the 0 padding\n",
    "embedding_size = 150\n",
    "output_size = len(labels)\n",
    "hidden_size = 256\n",
    "drop_prob = 0.5\n",
    "lr=0.001\n",
    "\n",
    "model = CharLSTM(vocab_size, embedding_size,  hidden_size, output_size, drop_prob, num_layers)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 40125.60154533386, Accuracy: 40.61%\n",
      "Epoch 2/5, Loss: 40013.02126789093, Accuracy: 44.71%\n",
      "Epoch 3/5, Loss: 39985.91010475159, Accuracy: 44.80%\n",
      "Epoch 4/5, Loss: 39966.6490354538, Accuracy: 45.41%\n",
      "Epoch 5/5, Loss: 39955.23589515686, Accuracy: 45.91%\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    for batch_sequences, batch_labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_sequences).float() # batch_size * seq_length * output_size\n",
    "        # convert batch_labels to one hot encoding\n",
    "        batch_labels_one_hot = F.one_hot(batch_labels, num_classes=output_size).float() # batch_size * seq_length * output_size\n",
    "        \n",
    "        loss = criterion(outputs, batch_labels_one_hot)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate accuracy TODO: make it on validation not on training\n",
    "        predicted_labels = outputs.argmax(dim=2)  # Get the index with the maximum probability\n",
    "        correct_predictions += (predicted_labels == batch_labels).sum().item()\n",
    "        total_predictions += batch_labels.numel()\n",
    "        \n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss}, Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_predict(model, sentence):\n",
    "    model.eval() # evaluation mode\n",
    "    sentence = [char_to_index[char] for char in sentence]\n",
    "    \n",
    "    # Create fixed-length sequences\n",
    "    fixed_sequences = [sentence[i:i+fixed_sequence_length] for i in range(0, len(sentence), fixed_sequence_length)]\n",
    "\n",
    "    # Pad 0 to last sequence if it is less than fixed_sequence_length\n",
    "    if len(fixed_sequences[-1]) < fixed_sequence_length:\n",
    "        fixed_sequences[-1] += [0] * (fixed_sequence_length - len(fixed_sequences[-1]))\n",
    "\n",
    "    sentence_sequences = torch.tensor(fixed_sequences).view(1, -1)  # Assuming batch size 1\n",
    "\n",
    "    print(sentence_sequences.shape)\n",
    "    outputs = model(sentence_sequences)\n",
    "    print(outputs.shape)\n",
    "    outputs = outputs.argmax(dim=2)\n",
    "    print(outputs.shape)\n",
    "    outputs = outputs.tolist()\n",
    "    print(outputs)\n",
    "    diacritics = []\n",
    "    for output in outputs:\n",
    "        for index in output:\n",
    "            predicted_class = indicies_to_labels[index]\n",
    "            if type(predicted_class) is tuple:\n",
    "                diacritics.append(chr(predicted_class[0]) + chr(predicted_class[1]))\n",
    "            elif predicted_class == 0:\n",
    "                diacritics.append('')\n",
    "            else:\n",
    "                diacritics.append(chr(predicted_class))\n",
    "    return diacritics[:len(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50])\n",
      "torch.Size([1, 50, 15])\n",
      "torch.Size([1, 50])\n",
      "[[3, 5, 3, 2, 1, 5, 1, 3, 3, 0, 5, 1, 5, 3, 12, 3, 3, 3, 12, 3, 1, 0, 3, 0, 5, 9, 5, 3, 3, 10, 3, 1, 5, 12, 7, 13, 11, 14, 8, 14, 14, 12, 12, 12, 14, 14, 12, 14, 12, 12]]\n",
      "قِوْلِهُأَوْقَطِعِالْأَوْلِيًّدِهِإِلًّخِقَالِالْزَّرْكِشِيُّ\n"
     ]
    }
   ],
   "source": [
    "test_sentence = 'قوله أو قطع الأول يده إلخ قال الزركشي'\n",
    "test_sentence = re.compile(r'[\\n+\\r+\\t+\\s+]').sub('', test_sentence)\n",
    "predicted_diacritics = lstm_predict(model, test_sentence)\n",
    "\n",
    "diacritized_sentence = ''\n",
    "for i in range(len(test_sentence)):\n",
    "    diacritized_sentence += test_sentence[i] + predicted_diacritics[i]\n",
    "\n",
    "print(diacritized_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
