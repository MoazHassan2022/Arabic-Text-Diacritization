{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "sys.path.insert(0, os.path.abspath('../..'))\n",
    "from preprocessing.preprocess import preprocess_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8351479\n",
      "421099\n",
      "15637321\n",
      "788621\n",
      "{'ن': 1, 'إ': 2, 'ذ': 3, 'ف': 4, 'ئ': 5, 'د': 6, 'ج': 7, 'ك': 8, 'أ': 9, 'ث': 10, 'م': 11, 'ط': 12, 'ت': 13, 'خ': 14, 'ؤ': 15, 'غ': 16, 'س': 17, 'ص': 18, 'ز': 19, 'ض': 20, '~': 21, 'ش': 22, 'ة': 23, 'ظ': 24, 'ق': 25, 'ا': 26, 'ح': 27, 'ب': 28, 'ي': 29, 'ه': 30, 'ر': 31, 'ء': 32, 'و': 33, 'ى': 34, 'ل': 35, 'آ': 36, 'ع': 37}\n",
      "{1: 'ن', 2: 'إ', 3: 'ذ', 4: 'ف', 5: 'ئ', 6: 'د', 7: 'ج', 8: 'ك', 9: 'أ', 10: 'ث', 11: 'م', 12: 'ط', 13: 'ت', 14: 'خ', 15: 'ؤ', 16: 'غ', 17: 'س', 18: 'ص', 19: 'ز', 20: 'ض', 21: '~', 22: 'ش', 23: 'ة', 24: 'ظ', 25: 'ق', 26: 'ا', 27: 'ح', 28: 'ب', 29: 'ي', 30: 'ه', 31: 'ر', 32: 'ء', 33: 'و', 34: 'ى', 35: 'ل', 36: 'آ', 37: 'ع'}\n"
     ]
    }
   ],
   "source": [
    "# lines limit\n",
    "limit = 30000\n",
    "dataset_path = '../../dataset'\n",
    "\n",
    "# preprocess and save the data\n",
    "preprocess_data(data_type='train', dataset_path=dataset_path)\n",
    "preprocess_data(data_type='val', dataset_path=dataset_path)\n",
    "\n",
    "# load data\n",
    "with open(f'{dataset_path}/cleaned_train_data_without_diacritics.txt', 'r', encoding='utf-8') as file:\n",
    "    # read all lines into a single string\n",
    "    training_data = re.compile(r'[\\n\\r\\t\\s]').sub('', file.read())\n",
    "with open(f'{dataset_path}/cleaned_val_data_without_diacritics.txt', 'r', encoding='utf-8') as file:\n",
    "    # read all lines into a single string\n",
    "    validation_data = re.compile(r'[\\n\\r\\t\\s]').sub('', file.read())\n",
    "    \n",
    "with open(f'{dataset_path}/cleaned_train_data_with_diacritics.txt', 'r', encoding='utf-8') as file:\n",
    "    # read all lines into a single string\n",
    "    training_data_with_diacritics = re.compile(r'[\\n\\r\\t\\s]').sub('', file.read())\n",
    "    \n",
    "with open(f'{dataset_path}/cleaned_val_data_with_diacritics.txt', 'r', encoding='utf-8') as file:\n",
    "    # read all lines into a single string\n",
    "    validation_data_with_diacritics = re.compile(r'[\\n\\r\\t\\s]').sub('', file.read())\n",
    "\n",
    "print(len(training_data))\n",
    "print(len(validation_data))\n",
    "print(len(training_data_with_diacritics))\n",
    "print(len(validation_data_with_diacritics))\n",
    "# Tokenize the text into sequences at the character level\n",
    "vocab = set(''.join(training_data + validation_data))\n",
    "\n",
    "char_to_index = {char: idx + 1 for idx, char in enumerate(vocab)}\n",
    "index_to_char = {idx + 1: char for idx, char in enumerate(vocab)}\n",
    "\n",
    "print(char_to_index)\n",
    "print(index_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"); print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1614: 1, 1615: 2, 1616: 3, 1617: 4, 1618: 5, 1611: 6, 1612: 7, 1613: 8, (1617, 1614): 9, (1617, 1615): 10, (1617, 1616): 11, (1617, 1611): 12, (1617, 1612): 13, (1617, 1613): 14}\n",
      "{0: 0, 1: 1614, 2: 1615, 3: 1616, 4: 1617, 5: 1618, 6: 1611, 7: 1612, 8: 1613, 9: (1617, 1614), 10: (1617, 1615), 11: (1617, 1616), 12: (1617, 1611), 13: (1617, 1612), 14: (1617, 1613)}\n"
     ]
    }
   ],
   "source": [
    "# define the diacritics unicode and their corresponding labels classes indices\n",
    "# note that index 0 is reserved for no diacritic\n",
    "labels = {\n",
    "    # no diacritic\n",
    "    0: 0,\n",
    "    # fath\n",
    "    1614: 1,\n",
    "    # damm\n",
    "    1615: 2,\n",
    "    # kasr\n",
    "    1616: 3,\n",
    "    # shadd\n",
    "    1617: 4,\n",
    "    # sukun\n",
    "    1618: 5,\n",
    "    # tanween bel fath\n",
    "    1611: 6,\n",
    "    # tanween bel damm\n",
    "    1612: 7,\n",
    "    # tanween bel kasr\n",
    "    1613: 8,\n",
    "    # shadd and fath\n",
    "    (1617, 1614): 9,\n",
    "    # shadd and damm\n",
    "    (1617, 1615): 10,\n",
    "    # shadd and kasr\n",
    "    (1617, 1616): 11,\n",
    "    # shadd and tanween bel fath\n",
    "    (1617, 1611): 12,\n",
    "    # shadd and tanween bel damm\n",
    "    (1617, 1612): 13,\n",
    "    # shadd and tanween bel kasr\n",
    "    (1617, 1613): 14\n",
    "}\n",
    "\n",
    "indicies_to_labels = {\n",
    "    # no diacritic\n",
    "    0: 0,\n",
    "    # fath\n",
    "    1: 1614,\n",
    "    # damm\n",
    "    2: 1615,\n",
    "    # kasr\n",
    "    3: 1616,\n",
    "    # shadd\n",
    "    4: 1617,\n",
    "    # sukun\n",
    "    5: 1618,\n",
    "    # tanween bel fath\n",
    "    6: 1611,\n",
    "    # tanween bel damm\n",
    "    7: 1612,\n",
    "    # tanween bel kasr\n",
    "    8: 1613,\n",
    "    # shadd and fath\n",
    "    9: (1617, 1614),\n",
    "    # shadd and damm\n",
    "    10: (1617, 1615),\n",
    "    # shadd and kasr\n",
    "    11: (1617, 1616),\n",
    "    # shadd and tanween bel fath\n",
    "    12: (1617, 1611),\n",
    "    # shadd and tanween bel damm\n",
    "    13: (1617, 1612),\n",
    "    # shadd and tanween bel kasr\n",
    "    14: (1617, 1613)\n",
    "}\n",
    "\n",
    "print(labels)\n",
    "print(indicies_to_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25, 33, 35, 30, 9, 33, 25, 12, 37, 26]\n",
      "8351479\n",
      "[25, 33, 35, 30, 33, 35, 26, 13, 8, 31]\n",
      "421099\n"
     ]
    }
   ],
   "source": [
    "# build one array that holds all sequences of training data\n",
    "training_data_sequences = [char_to_index[char] for char in training_data]\n",
    "print(training_data_sequences[:10])\n",
    "print(len(training_data_sequences))\n",
    "\n",
    "# build one array that holds all sequences of validation data\n",
    "validation_data_sequences = [char_to_index[char] for char in validation_data]\n",
    "print(validation_data_sequences[:10])\n",
    "print(len(validation_data_sequences))\n",
    "\n",
    "fixed_sequence_length = 50\n",
    "\n",
    "# Create fixed-length sequences\n",
    "fixed_sequences = [training_data_sequences[i:i+fixed_sequence_length] for i in range(0, len(training_data_sequences), fixed_sequence_length)]\n",
    "\n",
    "# Pad 0 to last sequence if it is less than fixed_sequence_length\n",
    "if len(fixed_sequences[-1]) < fixed_sequence_length:\n",
    "    fixed_sequences[-1] += [0] * (fixed_sequence_length - len(fixed_sequences[-1]))\n",
    "\n",
    "training_data_sequences = torch.tensor(fixed_sequences)\n",
    "\n",
    "fixed_sequence_length = 50\n",
    "\n",
    "# Create fixed-length sequences\n",
    "fixed_sequences = [validation_data_sequences[i:i+fixed_sequence_length] for i in range(0, len(validation_data_sequences), fixed_sequence_length)]\n",
    "\n",
    "# Pad 0 to last sequence if it is less than fixed_sequence_length\n",
    "if len(fixed_sequences[-1]) < fixed_sequence_length:\n",
    "    fixed_sequences[-1] += [0] * (fixed_sequence_length - len(fixed_sequences[-1]))\n",
    "\n",
    "validation_data_sequences = torch.tensor(fixed_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8351479\n",
      "[1, 5, 2, 2, 1, 5, 1, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "training_data_labels = []\n",
    "training_size = len(training_data_with_diacritics)\n",
    "index = 0\n",
    "while index < training_size:\n",
    "    if ord(training_data_with_diacritics[index]) not in labels:\n",
    "        # char is not a diacritic\n",
    "        if (index + 1) < training_size and ord(training_data_with_diacritics[index + 1]) in labels:\n",
    "            # char has a diacritic\n",
    "            if ord(training_data_with_diacritics[index + 1]) == 1617:\n",
    "                # char has a shadd diacritic\n",
    "                if (index + 2) < training_size and ord(training_data_with_diacritics[index + 2]) in labels:\n",
    "                    # char has a shadd and another diacritic\n",
    "                    training_data_labels.append(labels[(1617, ord(training_data_with_diacritics[index + 2]))])\n",
    "                    # skip next 2 diacritics chars\n",
    "                    index += 3  # increment by 3 to skip two diacritic chars\n",
    "                    continue\n",
    "                else:\n",
    "                    # char has a shadd and no other diacritic\n",
    "                    training_data_labels.append(labels[1617])\n",
    "                    # skip next diacritic char\n",
    "                    index += 2\n",
    "                    continue\n",
    "            # char has a diacritic other than shadd\n",
    "            training_data_labels.append(labels[ord(training_data_with_diacritics[index + 1])])\n",
    "            # skip next diacritic char\n",
    "            index += 2  # increment by 2 to skip one diacritic char\n",
    "            continue\n",
    "        else:\n",
    "            # char has no diacritic\n",
    "            training_data_labels.append(0)\n",
    "    index += 1  # increment by 1 for normal iteration\n",
    "\n",
    "print(len(training_data_labels))\n",
    "print(training_data_labels[:10])\n",
    "\n",
    "# Create fixed-length sequences\n",
    "fixed_labels = [training_data_labels[i:i+fixed_sequence_length] for i in range(0, len(training_data_labels), fixed_sequence_length)]\n",
    "\n",
    "# Pad 0 to last sequence if it is less than fixed_sequence_length\n",
    "if len(fixed_labels[-1]) < fixed_sequence_length:\n",
    "    fixed_labels[-1] += [0] * (fixed_sequence_length - len(fixed_labels[-1]))\n",
    "\n",
    "training_data_labels = torch.tensor(fixed_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421099\n",
      "[1, 5, 2, 2, 1, 1, 0, 2, 5, 1]\n"
     ]
    }
   ],
   "source": [
    "validation_data_labels = []\n",
    "validation_size = len(validation_data_with_diacritics)\n",
    "index = 0\n",
    "while index < validation_size:\n",
    "    if ord(validation_data_with_diacritics[index]) not in labels:\n",
    "        # char is not a diacritic\n",
    "        if (index + 1) < validation_size and ord(validation_data_with_diacritics[index + 1]) in labels:\n",
    "            # char has a diacritic\n",
    "            if ord(validation_data_with_diacritics[index + 1]) == 1617:\n",
    "                # char has a shadd diacritic\n",
    "                if (index + 2) < validation_size and ord(validation_data_with_diacritics[index + 2]) in labels:\n",
    "                    # char has a shadd and another diacritic\n",
    "                    validation_data_labels.append(labels[(1617, ord(validation_data_with_diacritics[index + 2]))])\n",
    "                    # skip next 2 diacritics chars\n",
    "                    index += 3  # increment by 3 to skip two diacritic chars\n",
    "                    continue\n",
    "                else:\n",
    "                    # char has a shadd and no other diacritic\n",
    "                    validation_data_labels.append(labels[1617])\n",
    "                    # skip next diacritic char\n",
    "                    index += 2\n",
    "                    continue\n",
    "            # char has a diacritic other than shadd\n",
    "            validation_data_labels.append(labels[ord(validation_data_with_diacritics[index + 1])])\n",
    "            # skip next diacritic char\n",
    "            index += 2  # increment by 2 to skip one diacritic char\n",
    "            continue\n",
    "        else:\n",
    "            # char has no diacritic\n",
    "            validation_data_labels.append(0)\n",
    "    index += 1  # increment by 1 for normal iteration\n",
    "\n",
    "print(len(validation_data_labels))\n",
    "print(validation_data_labels[:10])\n",
    "\n",
    "# Create fixed-length sequences\n",
    "fixed_labels = [validation_data_labels[i:i+fixed_sequence_length] for i in range(0, len(validation_data_labels), fixed_sequence_length)]\n",
    "\n",
    "# Pad 0 to last sequence if it is less than fixed_sequence_length\n",
    "if len(fixed_labels[-1]) < fixed_sequence_length:\n",
    "    fixed_labels[-1] += [0] * (fixed_sequence_length - len(fixed_labels[-1]))\n",
    "\n",
    "validation_data_labels = torch.tensor(fixed_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = TensorDataset(training_data_sequences, training_data_labels)\n",
    "\n",
    "batch_size = 32\n",
    "training_dataloader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "validation_dataset = TensorDataset(validation_data_sequences, validation_data_labels)\n",
    "\n",
    "batch_size = 32\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharLSTM(\n",
      "  (embedding): Embedding(38, 100)\n",
      "  (lstm): LSTM(100, 128, batch_first=True, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (output): Linear(in_features=256, out_features=15, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, output_size, drop_prob=0.5, num_layers=1):\n",
    "        super(CharLSTM, self).__init__()\n",
    "        # chars embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        \n",
    "        # LSTM layers\n",
    "        # batch_first: it means that the input tensor has its first dimension representing the batch size\n",
    "        self.lstm = nn.LSTM(input_size=embedding_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # Drop out layer, how likely would it drop some neurons (assign zeros to them)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        # output layer\n",
    "        self.output = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x) # batch_size * seq_length * embedding_size\n",
    "        lstm_out, _ = self.lstm(embedded) # batch_size * seq_length * hidden_size\n",
    "        after_dropout = self.dropout(lstm_out) # batch_size * seq_length *  hidden_size\n",
    "        output = self.output(after_dropout)  # batch_size * seq_length * output_size\n",
    "        output_softmax = F.softmax(output, dim=1)  # Apply softmax to the output\n",
    "        return output_softmax\n",
    "    \n",
    "num_layers = 1\n",
    "vocab_size = len(char_to_index) + 1 # +1 for the 0 padding\n",
    "embedding_size = 100\n",
    "output_size = len(labels)\n",
    "hidden_size = 128\n",
    "drop_prob = 0.5\n",
    "lr=0.001\n",
    "\n",
    "model = CharLSTM(vocab_size, embedding_size,  hidden_size, output_size, drop_prob, num_layers)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 66245.09496021271, Accuracy: 49.66%\n",
      "Epoch 2/20, Loss: 66216.53208637238, Accuracy: 50.51%\n",
      "Epoch 3/20, Loss: 66201.22449398041, Accuracy: 51.84%\n",
      "Epoch 4/20, Loss: 66189.92033958435, Accuracy: 52.38%\n",
      "Epoch 5/20, Loss: 66180.70656299591, Accuracy: 52.88%\n",
      "Epoch 6/20, Loss: 66174.84347248077, Accuracy: 52.59%\n",
      "Epoch 7/20, Loss: 66170.19342899323, Accuracy: 53.34%\n",
      "Epoch 8/20, Loss: 66165.47748661041, Accuracy: 53.37%\n",
      "Epoch 9/20, Loss: 66162.68735980988, Accuracy: 53.75%\n",
      "Epoch 10/20, Loss: 66159.15953445435, Accuracy: 53.89%\n",
      "Epoch 11/20, Loss: 66157.15152835846, Accuracy: 53.88%\n",
      "Epoch 12/20, Loss: 66153.86684703827, Accuracy: 54.18%\n",
      "Epoch 13/20, Loss: 66152.9382390976, Accuracy: 54.16%\n",
      "Epoch 14/20, Loss: 66150.04361724854, Accuracy: 54.50%\n",
      "Epoch 15/20, Loss: 66149.07486343384, Accuracy: 54.22%\n",
      "Epoch 16/20, Loss: 66147.85085773468, Accuracy: 54.57%\n",
      "Epoch 17/20, Loss: 66146.56024074554, Accuracy: 54.73%\n",
      "Epoch 18/20, Loss: 66145.20997238159, Accuracy: 54.73%\n",
      "Epoch 19/20, Loss: 66144.84865760803, Accuracy: 54.70%\n",
      "Epoch 20/20, Loss: 66143.90163040161, Accuracy: 54.78%\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    for batch_sequences, batch_labels in training_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_sequences).float() # batch_size * seq_length * output_size\n",
    "        # convert batch_labels to one hot encoding\n",
    "        batch_labels_one_hot = F.one_hot(batch_labels, num_classes=output_size).float() # batch_size * seq_length * output_size\n",
    "        \n",
    "        loss = criterion(outputs, batch_labels_one_hot)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    for validation_batch_sequences, validation_batch_labels in validation_dataloader:\n",
    "        outputs = model(validation_batch_sequences).float() # batch_size * seq_length * output_size\n",
    "        # Calculate accuracy\n",
    "        predicted_labels = outputs.argmax(dim=2)  # Get the index with the maximum probability\n",
    "        correct_predictions += (predicted_labels == validation_batch_labels).sum().item()\n",
    "        total_predictions += validation_batch_labels.numel()\n",
    "        \n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss}, Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model in pkl file\n",
    "with open(f'{dataset_path}/BiLSTM.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from pkl file\n",
    "del model\n",
    "with open(f'{dataset_path}/BiLSTM.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_predict(model, sentence):\n",
    "    model.eval() # evaluation mode\n",
    "    sentence = [char_to_index[char] for char in sentence]\n",
    "    \n",
    "    # Create fixed-length sequences\n",
    "    fixed_sequences = [sentence[i:i+fixed_sequence_length] for i in range(0, len(sentence), fixed_sequence_length)]\n",
    "\n",
    "    # Pad 0 to last sequence if it is less than fixed_sequence_length\n",
    "    if len(fixed_sequences[-1]) < fixed_sequence_length:\n",
    "        fixed_sequences[-1] += [0] * (fixed_sequence_length - len(fixed_sequences[-1]))\n",
    "\n",
    "    sentence_sequences = torch.tensor(fixed_sequences).view(1, -1)  # Assuming batch size 1\n",
    "\n",
    "    print(sentence_sequences.shape)\n",
    "    outputs = model(sentence_sequences)\n",
    "    print(outputs.shape)\n",
    "    outputs = outputs.argmax(dim=2)\n",
    "    print(outputs.shape)\n",
    "    outputs = outputs.tolist()\n",
    "    print(outputs)\n",
    "    diacritics = []\n",
    "    for output in outputs:\n",
    "        for index in output:\n",
    "            predicted_class = indicies_to_labels[index]\n",
    "            if type(predicted_class) is tuple:\n",
    "                diacritics.append(chr(predicted_class[0]) + chr(predicted_class[1]))\n",
    "            elif predicted_class == 0:\n",
    "                diacritics.append('')\n",
    "            else:\n",
    "                diacritics.append(chr(predicted_class))\n",
    "    return diacritics[:len(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50])\n",
      "torch.Size([1, 50, 15])\n",
      "torch.Size([1, 50])\n",
      "[[1, 5, 3, 2, 1, 5, 3, 5, 6, 0, 5, 1, 1, 3, 11, 3, 3, 3, 14, 8, 1, 0, 3, 0, 3, 9, 2, 3, 3, 10, 0, 3, 0, 14, 7, 14, 7, 14, 7, 0, 7, 12, 7, 13, 7, 13, 3, 12, 0, 12]]\n",
      "قَوْلِهُأَوْقِطْعًالْأَوَلِيِّدِهِإِلٍّخٍقَالِالِزَّرُكِشِيُّ\n"
     ]
    }
   ],
   "source": [
    "test_sentence = 'قوله أو قطع الأول يده إلخ قال الزركشي'\n",
    "test_sentence = re.compile(r'[\\n+\\r+\\t+\\s+]').sub('', test_sentence)\n",
    "predicted_diacritics = lstm_predict(model, test_sentence)\n",
    "\n",
    "diacritized_sentence = ''\n",
    "for i in range(len(test_sentence)):\n",
    "    diacritized_sentence += test_sentence[i] + predicted_diacritics[i]\n",
    "\n",
    "print(diacritized_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
