{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "sys.path.insert(0, os.path.abspath('../..'))\n",
    "from preprocessing.preprocess import preprocess_data, remove_diactrics\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_spaces(input_string):\n",
    "  space_count = len(re.findall(r'\\s', input_string))\n",
    "  return space_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54936\n",
      "2766\n",
      "54936\n",
      "2766\n",
      "قوله : أو قطع الأول يده إلخ قال الزركشي\n",
      "قَوْلُهُ : أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَخْ قَالَ الزَّرْكَشِيُّ\n"
     ]
    }
   ],
   "source": [
    "dataset_path = '../../dataset'\n",
    "\n",
    "limit = 2\n",
    "# preprocess and save the data\n",
    "preprocess_data(data_type='train', dataset_path=dataset_path)\n",
    "preprocess_data(data_type='val', dataset_path=dataset_path)\n",
    "\n",
    "max_len = 600\n",
    "# load data\n",
    "training_data = []\n",
    "training_spaces = []\n",
    "# load data\n",
    "with open(f'{dataset_path}/cleaned_train_data_without_diacritics.txt', 'r', encoding='utf-8') as file:\n",
    "    # read all lines into array of lines\n",
    "    training_data_lines = file.readlines()\n",
    "    for i in range(len(training_data_lines)):\n",
    "        training_data_lines[i] = re.compile(r'[\\n\\r\\t]').sub('', training_data_lines[i])\n",
    "        training_data_lines[i] = re.compile(r'\\s+').sub(' ', training_data_lines[i])\n",
    "        training_data_lines[i] = training_data_lines[i].strip()\n",
    "\n",
    "        dot_splitted_list = training_data_lines[i].split('.')\n",
    "        \n",
    "        # remove last string if empty\n",
    "        if dot_splitted_list[-1] == '':\n",
    "            dot_splitted_list = dot_splitted_list[:-1]\n",
    "        \n",
    "        for dot_splitted in dot_splitted_list:\n",
    "            dot_splitted = dot_splitted.strip()\n",
    "            # Split the line into sentences of max_len, without cutting words\n",
    "            sentences = textwrap.wrap(dot_splitted, max_len)\n",
    "\n",
    "            for sentence in sentences:\n",
    "                training_data.append(sentence)\n",
    "                training_spaces.append(count_spaces(sentence))\n",
    "                \n",
    "# validation\n",
    "validation_data = []\n",
    "validation_spaces = []\n",
    "# load data\n",
    "with open(f'{dataset_path}/cleaned_val_data_without_diacritics.txt', 'r', encoding='utf-8') as file:\n",
    "    # read all lines into array of lines\n",
    "    validation_data_lines = file.readlines()\n",
    "    for i in range(len(validation_data_lines)):\n",
    "        validation_data_lines[i] = re.compile(r'[\\n\\r\\t]').sub('', validation_data_lines[i])\n",
    "        validation_data_lines[i] = re.compile(r'\\s+').sub(' ', validation_data_lines[i])\n",
    "        validation_data_lines[i] = validation_data_lines[i].strip()\n",
    "\n",
    "        dot_splitted_list = validation_data_lines[i].split('.')\n",
    "        \n",
    "        # remove last string if empty\n",
    "        if dot_splitted_list[-1] == '':\n",
    "            dot_splitted_list = dot_splitted_list[:-1]\n",
    "        \n",
    "        for dot_splitted in dot_splitted_list:\n",
    "            dot_splitted = dot_splitted.strip()\n",
    "            # Split the line into sentences of max_len, without cutting words\n",
    "            sentences = textwrap.wrap(dot_splitted, max_len)\n",
    "\n",
    "            for sentence in sentences:\n",
    "                validation_data.append(sentence)\n",
    "                validation_spaces.append(count_spaces(sentence))\n",
    "\n",
    "\n",
    "training_data_with_diacritics = []\n",
    "spaces_index = 0\n",
    "\n",
    "with open(f'{dataset_path}/cleaned_train_data_with_diacritics.txt', 'r', encoding='utf-8') as file:\n",
    "    training_data_with_diacritics_lines = file.readlines()\n",
    "    for i in range(len(training_data_with_diacritics_lines)):\n",
    "        training_data_with_diacritics_lines[i] = re.compile(r'[\\n\\r\\t]').sub('', training_data_with_diacritics_lines[i])\n",
    "        training_data_with_diacritics_lines[i] = re.compile(r'\\s+').sub(' ', training_data_with_diacritics_lines[i])\n",
    "        training_data_with_diacritics_lines[i] = training_data_with_diacritics_lines[i].strip()\n",
    "        \n",
    "        dot_splitted_list = training_data_with_diacritics_lines[i].split('.')\n",
    "        \n",
    "        # remove last string if empty\n",
    "        if dot_splitted_list[-1] == '':\n",
    "            dot_splitted_list = dot_splitted_list[:-1]\n",
    "            \n",
    "        for dot_splitted in dot_splitted_list:\n",
    "            dot_splitted = dot_splitted.strip()\n",
    "            remaining = dot_splitted\n",
    "            remaining_length = len(remaining)\n",
    "            while(remaining_length > 0):\n",
    "                spaces_to_include = training_spaces[spaces_index]\n",
    "                spaces_index += 1\n",
    "                words = remaining.split()\n",
    "                if len(words) <= spaces_to_include + 1:\n",
    "                    training_data_with_diacritics.append(remaining.strip())\n",
    "                    remaining_length = 0\n",
    "                    break\n",
    "                else:\n",
    "                    sentence = ' '.join(words[:spaces_to_include + 1])\n",
    "                    training_data_with_diacritics.append(sentence.strip())\n",
    "                    remaining = ' '.join(words[spaces_to_include + 1:]).strip()\n",
    "                    remaining_length = len(remaining)\n",
    "\n",
    "validation_data_with_diacritics = []\n",
    "spaces_index = 0\n",
    "\n",
    "with open(f'{dataset_path}/cleaned_val_data_with_diacritics.txt', 'r', encoding='utf-8') as file:\n",
    "    validation_data_with_diacritics_lines = file.readlines()\n",
    "    for i in range(len(validation_data_with_diacritics_lines)):\n",
    "        validation_data_with_diacritics_lines[i] = re.compile(r'[\\n\\r\\t]').sub('', validation_data_with_diacritics_lines[i])\n",
    "        validation_data_with_diacritics_lines[i] = re.compile(r'\\s+').sub(' ', validation_data_with_diacritics_lines[i])\n",
    "        validation_data_with_diacritics_lines[i] = validation_data_with_diacritics_lines[i].strip()\n",
    "        \n",
    "        dot_splitted_list = validation_data_with_diacritics_lines[i].split('.')\n",
    "        \n",
    "        # remove last string if empty\n",
    "        if dot_splitted_list[-1] == '':\n",
    "            dot_splitted_list = dot_splitted_list[:-1]\n",
    "            \n",
    "        for dot_splitted in dot_splitted_list:\n",
    "            dot_splitted = dot_splitted.strip()\n",
    "            remaining = dot_splitted\n",
    "            remaining_length = len(remaining)\n",
    "            while(remaining_length > 0):\n",
    "                spaces_to_include = validation_spaces[spaces_index]\n",
    "                spaces_index += 1\n",
    "                words = remaining.split()\n",
    "                if len(words) <= spaces_to_include + 1:\n",
    "                    validation_data_with_diacritics.append(remaining.strip())\n",
    "                    remaining_length = 0\n",
    "                    break\n",
    "                else:\n",
    "                    sentence = ' '.join(words[:spaces_to_include + 1])\n",
    "                    validation_data_with_diacritics.append(sentence.strip())\n",
    "                    remaining = ' '.join(words[spaces_to_include + 1:]).strip()\n",
    "                    remaining_length = len(remaining)\n",
    "\n",
    "# test equality\n",
    "for i in range(len(training_data)):\n",
    "    if(training_data[i] != remove_diactrics([training_data_with_diacritics[i]])[0])  or len(training_data[i]) > max_len:\n",
    "        print('Diacritized text != cleaned text!')\n",
    "\n",
    "for i in range(len(validation_data)):\n",
    "    if validation_data[i] != remove_diactrics([validation_data_with_diacritics[i]])[0] or len(validation_data[i]) > max_len :\n",
    "        print('Diacritized text != cleaned text!')\n",
    "\n",
    "print(len(training_data))\n",
    "print(len(validation_data))\n",
    "print(len(training_data_with_diacritics))\n",
    "print(len(validation_data_with_diacritics))\n",
    "\n",
    "print(training_data[0])\n",
    "print(training_data_with_diacritics[0])\n",
    "\n",
    "## Tokenize the text into sequences at the character level\n",
    "char_to_index = {'ؤ': 1, 'ا': 2, 'ك': 3, 'ب': 4, 'ف': 5, 'و': 6, 'ث': 7, 'خ': 8, 'ظ': 9, 'أ': 10, 'ض': 11, 'س': 12, 'ج': 13, 'ى': 14, 'ص': 15, 'ع': 16, 'ذ': 17, 'ت': 18, ' ': 19, 'د': 20, 'ح': 21, 'ز': 22, 'ش': 23, 'إ': 24, 'ه': 25, 'ء': 26, 'م': 27, 'ق': 28, 'ة': 29, 'ن': 30, 'آ': 31, 'غ': 32, 'ي': 33, 'ر': 34, '!': 35, 'ل': 36, 'ئ': 37, 'ط': 38}\n",
    "index_to_char = {1: 'ؤ', 2: 'ا', 3: 'ك', 4: 'ب', 5: 'ف', 6: 'و', 7: 'ث', 8: 'خ', 9: 'ظ', 10: 'أ', 11: 'ض', 12: 'س', 13: 'ج', 14: 'ى', 15: 'ص', 16: 'ع', 17: 'ذ', 18: 'ت', 19: ' ', 20: 'د', 21: 'ح', 22: 'ز', 23: 'ش', 24: 'إ', 25: 'ه', 26: 'ء', 27: 'م', 28: 'ق', 29: 'ة', 30: 'ن', 31: 'آ', 32: 'غ', 33: 'ي', 34: 'ر', 35: '!', 36: 'ل', 37: 'ئ', 38: 'ط'}\n",
    "\n",
    "#for char, index in char_to_index.items():\n",
    "#    print(f'{char} => {index}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1614: 1, 1615: 2, 1616: 3, 1617: 4, 1618: 5, 1611: 6, 1612: 7, 1613: 8, (1617, 1614): 9, (1617, 1615): 10, (1617, 1616): 11, (1617, 1611): 12, (1617, 1612): 13, (1617, 1613): 14}\n",
      "{0: 0, 1: 1614, 2: 1615, 3: 1616, 4: 1617, 5: 1618, 6: 1611, 7: 1612, 8: 1613, 9: (1617, 1614), 10: (1617, 1615), 11: (1617, 1616), 12: (1617, 1611), 13: (1617, 1612), 14: (1617, 1613)}\n"
     ]
    }
   ],
   "source": [
    "# define the diacritics unicode and their corresponding labels classes indices\n",
    "# note that index 0 is reserved for no diacritic\n",
    "labels = {\n",
    "    # no diacritic\n",
    "    0: 0,\n",
    "    # fath\n",
    "    1614: 1,\n",
    "    # damm\n",
    "    1615: 2,\n",
    "    # kasr\n",
    "    1616: 3,\n",
    "    # shadd\n",
    "    1617: 4,\n",
    "    # sukun\n",
    "    1618: 5,\n",
    "    # tanween bel fath\n",
    "    1611: 6,\n",
    "    # tanween bel damm\n",
    "    1612: 7,\n",
    "    # tanween bel kasr\n",
    "    1613: 8,\n",
    "    # shadd and fath\n",
    "    (1617, 1614): 9,\n",
    "    # shadd and damm\n",
    "    (1617, 1615): 10,\n",
    "    # shadd and kasr\n",
    "    (1617, 1616): 11,\n",
    "    # shadd and tanween bel fath\n",
    "    (1617, 1611): 12,\n",
    "    # shadd and tanween bel damm\n",
    "    (1617, 1612): 13,\n",
    "    # shadd and tanween bel kasr\n",
    "    (1617, 1613): 14\n",
    "}\n",
    "\n",
    "indicies_to_labels = {\n",
    "    # no diacritic\n",
    "    0: 0,\n",
    "    # fath\n",
    "    1: 1614,\n",
    "    # damm\n",
    "    2: 1615,\n",
    "    # kasr\n",
    "    3: 1616,\n",
    "    # shadd\n",
    "    4: 1617,\n",
    "    # sukun\n",
    "    5: 1618,\n",
    "    # tanween bel fath\n",
    "    6: 1611,\n",
    "    # tanween bel damm\n",
    "    7: 1612,\n",
    "    # tanween bel kasr\n",
    "    8: 1613,\n",
    "    # shadd and fath\n",
    "    9: (1617, 1614),\n",
    "    # shadd and damm\n",
    "    10: (1617, 1615),\n",
    "    # shadd and kasr\n",
    "    11: (1617, 1616),\n",
    "    # shadd and tanween bel fath\n",
    "    12: (1617, 1611),\n",
    "    # shadd and tanween bel damm\n",
    "    13: (1617, 1612),\n",
    "    # shadd and tanween bel kasr\n",
    "    14: (1617, 1613)\n",
    "}\n",
    "\n",
    "print(labels)\n",
    "print(indicies_to_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"); print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40, 27, 11, 9, 24, 37, 24, 1, 27, 24]\n",
      "قوله : أو \n",
      "[24, 40, 27, 11, 9, 24, 37, 24, 27, 11]\n",
      " قوله : ول\n"
     ]
    }
   ],
   "source": [
    "# build one array that holds all sequences of training data\n",
    "training_data_sequences = [[char_to_index[char] for char in sequence] for sequence in training_data]\n",
    "\n",
    "# build one array that holds all sequences of validation data\n",
    "validation_data_sequences = [[char_to_index[char] for char in sequence] for sequence in validation_data]\n",
    "\n",
    "# Find the maximum sequence length\n",
    "max_len = max(max(len(sequence) for sequence in training_data_sequences), max(len(sequence) for sequence in validation_data_sequences))\n",
    "\n",
    "# Pad sequences to the maximum length\n",
    "training_data_sequences = [sequence + [0] * (max_len - len(sequence)) for sequence in training_data_sequences]\n",
    "\n",
    "# Pad sequences to the maximum length\n",
    "validation_data_sequences = [sequence + [0] * (max_len - len(sequence)) for sequence in validation_data_sequences]\n",
    "\n",
    "print(training_data_sequences[0][:10])\n",
    "print(training_data[0][:10])\n",
    "print(validation_data_sequences[0][:10])\n",
    "print(validation_data[0][:10])\n",
    "\n",
    "training_data_sequences = torch.tensor(training_data_sequences).to(device)\n",
    "\n",
    "\n",
    "validation_data_sequences = torch.tensor(validation_data_sequences).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "[1, 5, 2, 2, 0, 0, 0, 1, 5, 0]\n",
      "قوله : أو \n",
      "6420\n",
      "6420\n"
     ]
    }
   ],
   "source": [
    "training_data_labels = []\n",
    "training_size = len(training_data_with_diacritics)\n",
    "for sentence_index in range(training_size):\n",
    "  sentence_labels = []\n",
    "  sentence_size = len(training_data_with_diacritics[sentence_index])\n",
    "  index = 0\n",
    "  while index < sentence_size:\n",
    "      if ord(training_data_with_diacritics[sentence_index][index]) not in labels:\n",
    "          # char is not a diacritic\n",
    "          if (index + 1) < sentence_size and ord(training_data_with_diacritics[sentence_index][index + 1]) in labels:\n",
    "              # char has a diacritic\n",
    "              if ord(training_data_with_diacritics[sentence_index][index + 1]) == 1617:\n",
    "                  # char has a shadd diacritic\n",
    "                  if (index + 2) < sentence_size and ord(training_data_with_diacritics[sentence_index][index + 2]) in labels:\n",
    "                      # char has a shadd and another diacritic\n",
    "                      sentence_labels.append(labels[(1617, ord(training_data_with_diacritics[sentence_index][index + 2]))])\n",
    "                      # skip next 2 diacritics chars\n",
    "                      index += 3  # increment by 3 to skip two diacritic chars\n",
    "                      continue\n",
    "                  else:\n",
    "                      # char has a shadd and no other diacritic\n",
    "                      sentence_labels.append(labels[1617])\n",
    "                      # skip next diacritic char\n",
    "                      index += 2\n",
    "                      continue\n",
    "              # char has a diacritic other than shadd\n",
    "              sentence_labels.append(labels[ord(training_data_with_diacritics[sentence_index][index + 1])])\n",
    "              # skip next diacritic char\n",
    "              index += 2  # increment by 2 to skip one diacritic char\n",
    "              continue\n",
    "          else:\n",
    "              # char has no diacritic\n",
    "              sentence_labels.append(0)\n",
    "      index += 1  # increment by 1 for normal iteration\n",
    "\n",
    "  training_data_labels.append(sentence_labels)\n",
    "\n",
    "print(len(training_data_labels))\n",
    "print(training_data_labels[0][:10])\n",
    "print(training_data[0][:10])\n",
    "\n",
    "# Pad sequences to the maximum length\n",
    "training_data_labels = [sequence + [0] * (max_len - len(sequence)) for sequence in training_data_labels]\n",
    "\n",
    "print(len(training_data_labels) * len(training_data_labels[0]))\n",
    "print(len(training_data_sequences) * len(training_data_sequences[0]))\n",
    "\n",
    "training_data_labels = torch.tensor(training_data_labels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "[0, 1, 5, 2, 2, 0, 0, 0, 1, 1]\n",
      " قوله : ول\n",
      "6420\n",
      "6420\n"
     ]
    }
   ],
   "source": [
    "validation_data_labels = []\n",
    "validation_size = len(validation_data_with_diacritics)\n",
    "for sentence_index in range(validation_size):\n",
    "  sentence_labels = []\n",
    "  sentence_size = len(validation_data_with_diacritics[sentence_index])\n",
    "  index = 0\n",
    "  while index < sentence_size:\n",
    "      if ord(validation_data_with_diacritics[sentence_index][index]) not in labels:\n",
    "          # char is not a diacritic\n",
    "          if (index + 1) < sentence_size and ord(validation_data_with_diacritics[sentence_index][index + 1]) in labels:\n",
    "              # char has a diacritic\n",
    "              if ord(validation_data_with_diacritics[sentence_index][index + 1]) == 1617:\n",
    "                  # char has a shadd diacritic\n",
    "                  if (index + 2) < sentence_size and ord(validation_data_with_diacritics[sentence_index][index + 2]) in labels:\n",
    "                      # char has a shadd and another diacritic\n",
    "                      sentence_labels.append(labels[(1617, ord(validation_data_with_diacritics[sentence_index][index + 2]))])\n",
    "                      # skip next 2 diacritics chars\n",
    "                      index += 3  # increment by 3 to skip two diacritic chars\n",
    "                      continue\n",
    "                  else:\n",
    "                      # char has a shadd and no other diacritic\n",
    "                      sentence_labels.append(labels[1617])\n",
    "                      # skip next diacritic char\n",
    "                      index += 2\n",
    "                      continue\n",
    "              # char has a diacritic other than shadd\n",
    "              sentence_labels.append(labels[ord(validation_data_with_diacritics[sentence_index][index + 1])])\n",
    "              # skip next diacritic char\n",
    "              index += 2  # increment by 2 to skip one diacritic char\n",
    "              continue\n",
    "          else:\n",
    "              # char has no diacritic\n",
    "              sentence_labels.append(0)\n",
    "      index += 1  # increment by 1 for normal iteration\n",
    "\n",
    "  validation_data_labels.append(sentence_labels)\n",
    "\n",
    "print(len(validation_data_labels))\n",
    "print(validation_data_labels[0][:10])\n",
    "print(validation_data[0][:10])\n",
    "\n",
    "# Pad sequences to the maximum length\n",
    "validation_data_labels = [sequence + [0] * (max_len - len(sequence)) for sequence in validation_data_labels]\n",
    "\n",
    "print(len(validation_data_labels) * len(validation_data_labels[0]))\n",
    "print(len(validation_data_sequences) * len(validation_data_sequences[0]))\n",
    "\n",
    "validation_data_labels = torch.tensor(validation_data_labels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = TensorDataset(training_data_sequences, training_data_labels)\n",
    "\n",
    "batch_size = 1\n",
    "training_dataloader = DataLoader(training_dataset, batch_size=batch_size)\n",
    "\n",
    "validation_dataset = TensorDataset(validation_data_sequences, validation_data_labels)\n",
    "\n",
    "batch_size = 1\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharLSTM(\n",
      "  (embedding): Embedding(43, 2)\n",
      "  (lstm): LSTM(2, 2, batch_first=True, bidirectional=True)\n",
      "  (output): Linear(in_features=4, out_features=15, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, output_size, num_layers=1):\n",
    "        super(CharLSTM, self).__init__()\n",
    "        # chars embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "\n",
    "        # LSTM layers\n",
    "        # batch_first: it means that the input tensor has its first dimension representing the batch size\n",
    "        self.lstm = nn.LSTM(input_size=embedding_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # output layer\n",
    "        self.output = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x) # batch_size * seq_length * embedding_size\n",
    "        lstm_out, _ = self.lstm(embedded) # batch_size * seq_length * hidden_size\n",
    "        output = self.output(lstm_out)  # batch_size * seq_length * output_size\n",
    "        output_softmax = F.softmax(output, dim=1)  # Apply softmax to the output\n",
    "        return output_softmax\n",
    "\n",
    "num_layers = 1\n",
    "vocab_size = len(char_to_index) + 1 # +1 for the 0 padding\n",
    "embedding_size = 2\n",
    "output_size = len(labels)\n",
    "hidden_size = 2\n",
    "lr=0.001\n",
    "num_epochs = 5\n",
    "\n",
    "model = CharLSTM(vocab_size, embedding_size,  hidden_size, output_size, num_layers).to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/5, Accuracy: 16.63%\n",
      "Epoch 1/5, Loss: 866.4723424911499, Accuracy: 17.15%\n",
      "Epoch 2/5, Loss: 866.4669589996338, Accuracy: 17.45%\n",
      "Epoch 3/5, Loss: 866.461630821228, Accuracy: 17.79%\n",
      "Epoch 4/5, Loss: 866.4562888145447, Accuracy: 17.75%\n",
      "Epoch 5/5, Loss: 866.4509482383728, Accuracy: 17.96%\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# before training\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "for validation_batch_sequences, validation_batch_labels in validation_dataloader:\n",
    "    outputs = model(validation_batch_sequences).float() # batch_size * seq_length * output_size\n",
    "\n",
    "    # Calculate accuracy using mask\n",
    "    mask = (validation_batch_sequences != 0).float()\n",
    "\n",
    "    # Calculate accuracy\n",
    "    predicted_labels = outputs.argmax(dim=2)  # Get the index with the maximum probability\n",
    "\n",
    "    # Only consider non-padded elements in accuracy calculation\n",
    "    correct_predictions += ((predicted_labels == validation_batch_labels) * mask).sum().item()\n",
    "    total_predictions += mask.sum().item()\n",
    "\n",
    "\n",
    "accuracy = correct_predictions / total_predictions\n",
    "print(f'Epoch 0/{num_epochs}, Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    for batch_sequences, batch_labels in training_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_sequences).float() # batch_size * seq_length * output_size\n",
    "        # convert batch_labels to one hot encoding\n",
    "        batch_labels_one_hot = F.one_hot(batch_labels, num_classes=output_size).float() # batch_size * seq_length * output_size\n",
    "\n",
    "        # Calculate the mask\n",
    "        mask = (batch_sequences != 0).float()\n",
    "\n",
    "        # Apply the mask to both the outputs and the labels\n",
    "        masked_outputs = outputs * mask.unsqueeze(2)\n",
    "        masked_labels = batch_labels_one_hot * mask.unsqueeze(2)\n",
    "\n",
    "        loss = criterion(masked_outputs, masked_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "\n",
    "    for validation_batch_sequences, validation_batch_labels in validation_dataloader:\n",
    "        outputs = model(validation_batch_sequences).float() # batch_size * seq_length * output_size\n",
    "        # Calculate accuracy using mask\n",
    "        mask = (validation_batch_sequences != 0).float()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        predicted_labels = outputs.argmax(dim=2)  # Get the index with the maximum probability\n",
    "\n",
    "        # Only consider non-padded elements in accuracy calculation\n",
    "        correct_predictions += ((predicted_labels == validation_batch_labels) * mask).sum().item()\n",
    "        total_predictions += mask.sum().item()\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss}, Accuracy: {accuracy * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
